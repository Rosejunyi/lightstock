#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸‹è½½ä¸å¤æƒKçº¿æ•°æ® v2.1 - å¢é‡ä¸‹è½½ç‰ˆï¼ˆå«å®Œæ•´å­—æ®µï¼‰

æ–°å¢åŠŸèƒ½ï¼š
1. âœ… è‡ªåŠ¨æ£€æµ‹ï¼šè¯»å–ç°æœ‰æ•°æ®çš„æœ€æ–°æ—¥æœŸ
2. âœ… å¢é‡ä¸‹è½½ï¼šåªä¸‹è½½ç¼ºå¤±çš„äº¤æ˜“æ—¥æ•°æ®
3. âœ… æ™ºèƒ½åˆå¹¶ï¼šè‡ªåŠ¨å»é‡ã€æ’åºã€è¡¥å…¨
4. âœ… æ•°æ®éªŒè¯ï¼šæ£€æŸ¥æ¢æ‰‹ç‡å­—æ®µå’Œæ•°æ®å®Œæ•´æ€§
5. âœ… è‡ªåŠ¨å¤‡ä»½ï¼šä¿ç•™å†å²ç‰ˆæœ¬
6. âœ… æ–­ç‚¹ç»­ä¼ ï¼šæ”¯æŒä¸­æ–­åç»§ç»­
7. âœ… å®Œæ•´å­—æ®µï¼šæ¶¨è·Œé¢ã€æŒ¯å¹…ã€æ˜¨æ”¶ã€æ¶¨è·Œå¹…å¼‚å¸¸ã€åœç‰Œ

é€‚ç”¨åœºæ™¯ï¼š
- æ¯æ—¥å®šæ—¶æ›´æ–°Kçº¿æ•°æ®
- è‡ªåŠ¨è¡¥å…¨èŠ‚å‡æ—¥åçš„ç¼ºå¤±æ•°æ®
- ä¿æŒæ•°æ®æœ€æ–°ä¸”é¿å…é‡å¤ä¸‹è½½
- ç¡®ä¿åŒ…å«æ¢æ‰‹ç‡å’Œæ‰€æœ‰æ´¾ç”Ÿå­—æ®µ

ä½œè€…ï¼šClaude
ç‰ˆæœ¬ï¼šv2.1
æ—¥æœŸï¼š2025-11-03
"""

import baostock as bs
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
from tqdm import tqdm
import logging
import time
import shutil

# ============================================================
# é…ç½®
# ============================================================

# ç›®å½•é…ç½®
OUTPUT_DIR = Path("data/daily_kline_raw")  # ä¸å¤æƒKçº¿æ•°æ®
STOCK_INFO_FILE = Path("data/stock_basic_info.parquet")  # è‚¡ç¥¨ä¿¡æ¯
BACKUP_DIR = Path("data/backups/kline_raw")
LOG_DIR = Path("logs")

# åˆ›å»ºç›®å½•
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
BACKUP_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)

# ä¸‹è½½é…ç½®
DEFAULT_START_DATE = "1990-01-01"  # é¦–æ¬¡ä¸‹è½½çš„å¼€å§‹æ—¥æœŸ
END_DATE = datetime.now().strftime('%Y-%m-%d')  # ç»“æŸæ—¥æœŸï¼ˆä»Šå¤©ï¼‰
MAX_RETRIES = 3
RETRY_DELAY = 2
BATCH_SIZE = 50

# å¢é‡æ›´æ–°é…ç½®
INCREMENTAL_CONFIG = {
    'force_full_download': False,     # æ˜¯å¦å¼ºåˆ¶å…¨é‡ä¸‹è½½
    'lookback_days': 10,              # å›æº¯å¤©æ•°ï¼šé˜²æ­¢æ•°æ®é—æ¼
    'min_gap_days': 1,                # æœ€å°æ›´æ–°é—´éš”ï¼šè·ç¦»æœ€æ–°æ•°æ®<Nå¤©ä¸æ›´æ–°
    'backup_before_update': True,     # æ›´æ–°å‰æ˜¯å¦å¤‡ä»½
}

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(
            LOG_DIR / f'kline_raw_incremental_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
            encoding='utf-8'
        ),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================

def add_market_prefix(pure_code):
    """æ·»åŠ å¸‚åœºå‰ç¼€"""
    pure_code = str(pure_code).zfill(6)
    if pure_code.startswith('6') or pure_code.startswith('5'):
        return f'sh.{pure_code}'
    else:
        return f'sz.{pure_code}'

def extract_pure_code(code_with_prefix):
    """æå–çº¯æ•°å­—è‚¡ç¥¨ä»£ç """
    if pd.isna(code_with_prefix):
        return None
    code_str = str(code_with_prefix)
    if '.' in code_str:
        return code_str.split('.')[1]
    return code_str

def format_date_string(date_value):
    """æ ¼å¼åŒ–æ—¥æœŸä¸º YYYY-MM-DD å­—ç¬¦ä¸²"""
    if pd.isna(date_value):
        return None
    try:
        if isinstance(date_value, str):
            if len(date_value) == 10 and date_value[4] == '-' and date_value[7] == '-':
                return date_value
        dt = pd.to_datetime(date_value)
        return dt.strftime('%Y-%m-%d')
    except:
        return None

def calculate_derived_fields(df):
    """
    è®¡ç®—æ´¾ç”Ÿå­—æ®µ
    
    æ–°å¢å­—æ®µï¼š
    - æ¶¨è·Œé¢ï¼šæ”¶ç›˜ - æ˜¨æ”¶
    - æŒ¯å¹…ï¼š(æœ€é«˜ - æœ€ä½) / æ˜¨æ”¶ * 100
    - æ˜¨æ”¶ï¼šå‰ä¸€å¤©çš„æ”¶ç›˜ä»·ï¼ˆå¦‚æœAPIæ²¡æœ‰æä¾›ï¼‰
    - æ¶¨è·Œå¹…å¼‚å¸¸ï¼šæ¶¨è·Œå¹…è¶…è¿‡Â±10%
    - åœç‰Œï¼šæˆäº¤é‡ä¸º0
    """
    if df.empty:
        return df
    
    # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
    df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
    
    # å¦‚æœAPIæ²¡æœ‰æä¾›æ˜¨æ”¶ï¼Œä½¿ç”¨å‰ä¸€å¤©çš„æ”¶ç›˜ä»·è®¡ç®—
    if 'æ˜¨æ”¶' not in df.columns or df['æ˜¨æ”¶'].isna().all():
        df['æ˜¨æ”¶'] = df['æ”¶ç›˜'].shift(1)
    
    # è®¡ç®—æ¶¨è·Œé¢
    df['æ¶¨è·Œé¢'] = df['æ”¶ç›˜'] - df['æ˜¨æ”¶']
    
    # è®¡ç®—æŒ¯å¹…
    df['æŒ¯å¹…'] = ((df['æœ€é«˜'] - df['æœ€ä½']) / df['æ˜¨æ”¶'] * 100).round(4)
    
    # åˆ¤æ–­æ¶¨è·Œå¹…å¼‚å¸¸ï¼ˆæ¶¨è·Œå¹…è¶…è¿‡Â±10%ï¼Œæˆ–STè‚¡è¶…è¿‡Â±5%ï¼‰
    df['æ¶¨è·Œå¹…å¼‚å¸¸'] = df['æ¶¨è·Œå¹…'].apply(
        lambda x: 'X' if (pd.notna(x) and (abs(x) > 10)) else None
    )
    
    # åœç‰Œåˆ¤æ–­ï¼ˆæˆäº¤é‡ä¸º0è§†ä¸ºåœç‰Œï¼‰
    df['åœç‰Œ'] = df['æˆäº¤é‡'].apply(lambda x: 'X' if (pd.notna(x) and x == 0) else None)
    
    return df

def get_existing_data(stock_code):
    """
    è¯»å–ç°æœ‰æ•°æ®
    
    è¿”å›: (DataFrame, æœ€æ–°æ—¥æœŸ)
    """
    file_path = OUTPUT_DIR / f"{stock_code}.parquet"
    
    if not file_path.exists():
        return None, None
    
    try:
        df = pd.read_parquet(file_path)
        
        if df.empty:
            return None, None
        
        # è·å–æœ€æ–°æ—¥æœŸ
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        latest_date = df['æ—¥æœŸ'].max().strftime('%Y-%m-%d')
        
        logger.debug(f"{stock_code}: ç°æœ‰æ•°æ® {len(df)} æ¡ï¼Œæœ€æ–°æ—¥æœŸ {latest_date}")
        
        return df, latest_date
    
    except Exception as e:
        logger.error(f"{stock_code}: è¯»å–ç°æœ‰æ•°æ®å¤±è´¥ - {e}")
        return None, None

def calculate_download_range(latest_date, stock_code):
    """
    è®¡ç®—éœ€è¦ä¸‹è½½çš„æ—¥æœŸèŒƒå›´
    
    è¿”å›: (å¼€å§‹æ—¥æœŸ, ç»“æŸæ—¥æœŸ, æ˜¯å¦éœ€è¦ä¸‹è½½)
    """
    if INCREMENTAL_CONFIG['force_full_download']:
        return DEFAULT_START_DATE, END_DATE, True
    
    if latest_date is None:
        # é¦–æ¬¡ä¸‹è½½
        return DEFAULT_START_DATE, END_DATE, True
    
    # è§£ææœ€æ–°æ—¥æœŸ
    latest_dt = datetime.strptime(latest_date, '%Y-%m-%d')
    today = datetime.now()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    days_gap = (today - latest_dt).days
    
    if days_gap <= INCREMENTAL_CONFIG['min_gap_days']:
        logger.debug(f"{stock_code}: æ•°æ®å·²æ˜¯æœ€æ–°ï¼ˆè·ä»Š{days_gap}å¤©ï¼‰ï¼Œè·³è¿‡ä¸‹è½½")
        return None, None, False
    
    # è®¡ç®—ä¸‹è½½èŒƒå›´ï¼ˆå›æº¯Nå¤©é˜²æ­¢é—æ¼ï¼‰
    start_dt = latest_dt - timedelta(days=INCREMENTAL_CONFIG['lookback_days'])
    start_date = start_dt.strftime('%Y-%m-%d')
    
    logger.debug(f"{stock_code}: å¢é‡ä¸‹è½½ {start_date} è‡³ {END_DATE}")
    
    return start_date, END_DATE, True

def download_kline_data(stock_code, start_date=None, end_date=None, retry_count=0):
    """
    ä¸‹è½½å•åªè‚¡ç¥¨çš„ä¸å¤æƒKçº¿æ•°æ®
    
    å‚æ•°:
        stock_code: è‚¡ç¥¨ä»£ç ï¼ˆ6ä½æ•°å­—ï¼‰
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        retry_count: å½“å‰é‡è¯•æ¬¡æ•°
    
    è¿”å›: 
        DataFrame æˆ– None
    """
    try:
        # æ·»åŠ å¸‚åœºå‰ç¼€
        bs_code = add_market_prefix(stock_code)
        
        # ä½¿ç”¨ä¼ å…¥çš„æ—¥æœŸèŒƒå›´
        if start_date is None:
            start_date = DEFAULT_START_DATE
        if end_date is None:
            end_date = END_DATE
        
        # è°ƒç”¨ Baostock API - è¯·æ±‚å®Œæ•´å­—æ®µ
        rs = bs.query_history_k_data_plus(
            code=bs_code,
            fields="date,code,open,high,low,close,preclose,volume,amount,turn,pctChg,isST",
            start_date=start_date,
            end_date=end_date,
            frequency="d",
            adjustflag="3"  # â­ "3" = ä¸å¤æƒ
        )
        
        # æ£€æŸ¥è¿”å›çŠ¶æ€
        if rs.error_code != '0':
            if retry_count < MAX_RETRIES:
                logger.debug(f"{stock_code}: ä¸‹è½½å¤±è´¥ï¼Œé‡è¯• {retry_count + 1}/{MAX_RETRIES}")
                time.sleep(RETRY_DELAY)
                return download_kline_data(stock_code, start_date, end_date, retry_count + 1)
            else:
                logger.debug(f"{stock_code}: {rs.error_msg}")
                return None
        
        # æ”¶é›†æ•°æ®
        data_list = []
        while (rs.error_code == '0') & rs.next():
            data_list.append(rs.get_row_data())
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if not data_list:
            logger.debug(f"{stock_code}: æ— Kçº¿æ•°æ®")
            return pd.DataFrame()  # è¿”å›ç©ºDataFrame
        
        # è½¬æ¢ä¸º DataFrame
        df = pd.DataFrame(data_list, columns=rs.fields)
        
        # é‡å‘½ååˆ—
        column_mapping = {
            'date': 'æ—¥æœŸ',
            'code': 'ä»£ç ',
            'open': 'å¼€ç›˜',
            'high': 'æœ€é«˜',
            'low': 'æœ€ä½',
            'close': 'æ”¶ç›˜',
            'preclose': 'æ˜¨æ”¶',
            'volume': 'æˆäº¤é‡',
            'amount': 'æˆäº¤é¢',
            'turn': 'æ¢æ‰‹ç‡',
            'pctChg': 'æ¶¨è·Œå¹…',
            'isST': 'STæ ‡è®°'
        }
        
        df = df.rename(columns=column_mapping)
        
        # æå–çº¯æ•°å­—ä»£ç 
        df['è‚¡ç¥¨ä»£ç '] = df['ä»£ç '].apply(extract_pure_code)
        df = df.drop(columns=['ä»£ç '])
        
        # è½¬æ¢æ•°æ®ç±»å‹
        numeric_columns = ['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æ˜¨æ”¶', 'æˆäº¤é¢', 'æ¢æ‰‹ç‡', 'æ¶¨è·Œå¹…']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        if 'æˆäº¤é‡' in df.columns:
            df['æˆäº¤é‡'] = pd.to_numeric(df['æˆäº¤é‡'], errors='coerce').astype('Int64')
        
        # æ ¼å¼åŒ–æ—¥æœŸ
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].apply(format_date_string)
        
        # åˆ é™¤æ— æ•ˆè¡Œ
        df = df.dropna(subset=['æ—¥æœŸ', 'æ”¶ç›˜'])
        
        if df.empty:
            logger.debug(f"{stock_code}: è½¬æ¢åæ— æœ‰æ•ˆæ•°æ®")
            return pd.DataFrame()
        
        # è®¡ç®—æ´¾ç”Ÿå­—æ®µ
        df = calculate_derived_fields(df)
        
        return df
    
    except Exception as e:
        if retry_count < MAX_RETRIES:
            logger.debug(f"{stock_code}: å¼‚å¸¸ï¼Œé‡è¯• {retry_count + 1}/{MAX_RETRIES} - {e}")
            time.sleep(RETRY_DELAY)
            return download_kline_data(stock_code, start_date, end_date, retry_count + 1)
        else:
            logger.error(f"{stock_code}: ä¸‹è½½å¼‚å¸¸ - {e}")
            return None

def merge_data(df_existing, df_new, stock_code, stock_info):
    """
    åˆå¹¶ç°æœ‰æ•°æ®å’Œæ–°ä¸‹è½½çš„æ•°æ®
    
    ç­–ç•¥ï¼š
    1. åˆå¹¶ä¸¤ä¸ªDataFrame
    2. æŒ‰æ—¥æœŸå»é‡ï¼ˆä¿ç•™æ–°æ•°æ®ï¼‰
    3. é‡æ–°è®¡ç®—æ‰€æœ‰æ´¾ç”Ÿå­—æ®µï¼ˆç¡®ä¿æ•°æ®ä¸€è‡´æ€§ï¼‰
    4. æ’åº
    5. æ·»åŠ è‚¡ç¥¨ä¿¡æ¯
    """
    if df_existing is None or df_existing.empty:
        df_result = df_new.copy()
        # Ensure date is datetime type
        if not pd.api.types.is_datetime64_any_dtype(df_result['æ—¥æœŸ']):
            df_result['æ—¥æœŸ'] = pd.to_datetime(df_result['æ—¥æœŸ'])
    elif df_new is None or df_new.empty:
        return df_existing
    else:
        # ç¡®ä¿æ—¥æœŸæ ¼å¼ä¸€è‡´
        df_existing['æ—¥æœŸ'] = pd.to_datetime(df_existing['æ—¥æœŸ'])
        df_new['æ—¥æœŸ'] = pd.to_datetime(df_new['æ—¥æœŸ'])
        
        # åˆå¹¶
        df_result = pd.concat([df_existing, df_new], ignore_index=True)
        
        # å»é‡ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
        df_result = df_result.drop_duplicates(subset=['æ—¥æœŸ'], keep='last')
        
        # æ’åº
        df_result = df_result.sort_values('æ—¥æœŸ').reset_index(drop=True)
    
    # Convert date back to string (ensure date column is datetime type)
    if pd.api.types.is_datetime64_any_dtype(df_result['æ—¥æœŸ']):
        df_result['æ—¥æœŸ'] = df_result['æ—¥æœŸ'].dt.strftime('%Y-%m-%d')
    else:
        # If not datetime type, convert first then format
        df_result['æ—¥æœŸ'] = pd.to_datetime(df_result['æ—¥æœŸ']).dt.strftime('%Y-%m-%d')
    
    # é‡æ–°è®¡ç®—æ´¾ç”Ÿå­—æ®µï¼ˆç¡®ä¿å®Œæ•´æ€§ï¼‰
    df_result = calculate_derived_fields(df_result)
    
    # ç¡®ä¿è‚¡ç¥¨ä»£ç æ­£ç¡®
    df_result['è‚¡ç¥¨ä»£ç '] = stock_code
    
    # æ·»åŠ è‚¡ç¥¨åç§°
    if stock_info is not None:
        stock_row = stock_info[stock_info['è‚¡ç¥¨ä»£ç '] == stock_code]
        if not stock_row.empty:
            df_result['è‚¡ç¥¨åç§°'] = stock_row['è‚¡ç¥¨åç§°'].iloc[0]
    
    return df_result

def get_stock_list_from_baostock():
    """ä»Baostockè·å–æ‰€æœ‰Aè‚¡è‚¡ç¥¨åˆ—è¡¨"""
    logger.info("ä»Baostockè·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨...")
    
    try:
        rs = bs.query_stock_basic()
        
        if rs.error_code != '0':
            logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {rs.error_msg}")
            return []
        
        stock_list = []
        while (rs.error_code == '0') & rs.next():
            stock_list.append(rs.get_row_data())
        
        if not stock_list:
            logger.error("è‚¡ç¥¨åˆ—è¡¨ä¸ºç©º")
            return []
        
        df = pd.DataFrame(stock_list, columns=rs.fields)
        df = df[df['code'].str.contains(r'(sh\.[65]|sz\.[03])', na=False)]
        df['pure_code'] = df['code'].apply(extract_pure_code)
        
        # è¿‡æ»¤é€€å¸‚è‚¡ç¥¨
        if 'status' in df.columns:
            df = df[df['status'] == '1']
        
        stock_codes = df['pure_code'].tolist()
        logger.info(f"âœ… è·å–åˆ° {len(stock_codes)} åªAè‚¡")
        
        return stock_codes
    
    except Exception as e:
        logger.error(f"è·å–è‚¡ç¥¨åˆ—è¡¨å¼‚å¸¸: {e}")
        return []

def get_stock_list():
    """è·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆä¼˜å…ˆä»å·²æœ‰æ•°æ®ï¼Œå…¶æ¬¡ä»Baostockï¼‰"""
    # ä¼˜å…ˆä»å·²æœ‰æ•°æ®ä¸­è·å–
    if OUTPUT_DIR.exists():
        existing_files = list(OUTPUT_DIR.glob("*.parquet"))
        if existing_files:
            stock_codes = [f.stem for f in existing_files]
            logger.info(f"âœ… ä»å·²æœ‰æ•°æ®è·å–åˆ° {len(stock_codes)} åªè‚¡ç¥¨")
            return stock_codes
    
    # ä»è‚¡ç¥¨ä¿¡æ¯æ–‡ä»¶è·å–
    if STOCK_INFO_FILE.exists():
        try:
            stock_info = pd.read_parquet(STOCK_INFO_FILE)
            stock_codes = stock_info['è‚¡ç¥¨ä»£ç '].astype(str).str.zfill(6).tolist()
            logger.info(f"âœ… ä»è‚¡ç¥¨ä¿¡æ¯æ–‡ä»¶è·å–åˆ° {len(stock_codes)} åªè‚¡ç¥¨")
            return stock_codes
        except Exception as e:
            logger.warning(f"è¯»å–è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {e}")
    
    # æœ€åä»Baostockè·å–
    return get_stock_list_from_baostock()

def load_stock_info():
    """åŠ è½½è‚¡ç¥¨ä¿¡æ¯"""
    try:
        if not STOCK_INFO_FILE.exists():
            logger.warning(f"âš ï¸  è‚¡ç¥¨ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {STOCK_INFO_FILE}")
            return None
        
        logger.info(f"åŠ è½½è‚¡ç¥¨ä¿¡æ¯: {STOCK_INFO_FILE}")
        stock_info = pd.read_parquet(STOCK_INFO_FILE)
        
        # ç¡®ä¿è‚¡ç¥¨ä»£ç æ˜¯å­—ç¬¦ä¸²æ ¼å¼
        if 'è‚¡ç¥¨ä»£ç ' in stock_info.columns:
            stock_info['è‚¡ç¥¨ä»£ç '] = stock_info['è‚¡ç¥¨ä»£ç '].astype(str).str.zfill(6)
        
        logger.info(f"âœ… è‚¡ç¥¨ä¿¡æ¯æ•°æ®: {len(stock_info)} åªè‚¡ç¥¨")
        
        return stock_info
    except Exception as e:
        logger.error(f"âŒ åŠ è½½è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {e}")
        return None

def backup_file(file_path):
    """å¤‡ä»½å•ä¸ªæ–‡ä»¶"""
    if not file_path.exists():
        return
    
    try:
        stock_code = file_path.stem
        backup_subdir = BACKUP_DIR / stock_code
        backup_subdir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = backup_subdir / f"{stock_code}_{timestamp}.parquet"
        
        shutil.copy2(file_path, backup_file)
        logger.debug(f"å·²å¤‡ä»½: {backup_file}")
        
        # æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘3ä¸ªï¼‰
        backups = sorted(backup_subdir.glob(f"{stock_code}_*.parquet"), reverse=True)
        for old_backup in backups[3:]:
            old_backup.unlink()
            logger.debug(f"åˆ é™¤æ—§å¤‡ä»½: {old_backup}")
    
    except Exception as e:
        logger.warning(f"å¤‡ä»½å¤±è´¥: {e}")

def validate_turnover_field(df, stock_code):
    """éªŒè¯æ¢æ‰‹ç‡å­—æ®µ"""
    if 'æ¢æ‰‹ç‡' not in df.columns:
        logger.warning(f"{stock_code}: ç¼ºå°‘æ¢æ‰‹ç‡å­—æ®µï¼")
        return False
    
    turnover_valid = df['æ¢æ‰‹ç‡'].notna().sum()
    turnover_total = len(df)
    completeness = turnover_valid / turnover_total if turnover_total > 0 else 0
    
    if completeness < 0.5:  # å°‘äº50%æœ‰æ•ˆæ•°æ®
        logger.warning(f"{stock_code}: æ¢æ‰‹ç‡æ•°æ®ä¸å®Œæ•´ ({completeness*100:.1f}%)")
        return False
    
    return True

# ============================================================
# ä¸»ç¨‹åº
# ============================================================

def main():
    print("=" * 80)
    print("  ä¸‹è½½ä¸å¤æƒKçº¿æ•°æ® v2.1 - å¢é‡ä¸‹è½½ç‰ˆï¼ˆå«å®Œæ•´å­—æ®µï¼‰")
    print("  æ™ºèƒ½æ£€æµ‹ + å¢é‡æ›´æ–° + å®Œæ•´å­—æ®µ + æ•°æ®éªŒè¯")
    print("=" * 80)
    
    print(f"\né…ç½®:")
    print(f"  å¼ºåˆ¶å…¨é‡ä¸‹è½½: {'æ˜¯' if INCREMENTAL_CONFIG['force_full_download'] else 'å¦'}")
    print(f"  å›æº¯å¤©æ•°: {INCREMENTAL_CONFIG['lookback_days']} å¤©")
    print(f"  æœ€å°æ›´æ–°é—´éš”: {INCREMENTAL_CONFIG['min_gap_days']} å¤©")
    print(f"  ç»“æŸæ—¥æœŸ: {END_DATE}")
    print(f"  æ›´æ–°å‰å¤‡ä»½: {'æ˜¯' if INCREMENTAL_CONFIG['backup_before_update'] else 'å¦'}")
    
    print(f"\nâœ¨ å®Œæ•´å­—æ®µæ”¯æŒ:")
    print(f"  âœ… åŸºç¡€å­—æ®µï¼šå¼€ç›˜ã€æœ€é«˜ã€æœ€ä½ã€æ”¶ç›˜ã€æˆäº¤é‡ã€æˆäº¤é¢ã€æ¢æ‰‹ç‡ã€æ¶¨è·Œå¹…")
    print(f"  âœ… æ´¾ç”Ÿå­—æ®µï¼šæ¶¨è·Œé¢ã€æŒ¯å¹…ã€æ˜¨æ”¶ã€æ¶¨è·Œå¹…å¼‚å¸¸ã€åœç‰Œ")
    
    # åŠ è½½è‚¡ç¥¨ä¿¡æ¯
    print("\næ­¥éª¤ 1/4: åŠ è½½è‚¡ç¥¨ä¿¡æ¯...")
    stock_info = load_stock_info()
    if stock_info is not None:
        print(f"âœ… è‚¡ç¥¨ä¿¡æ¯åŠ è½½æˆåŠŸ: {len(stock_info)} åªè‚¡ç¥¨")
    else:
        print(f"âš ï¸  æœªåŠ è½½è‚¡ç¥¨ä¿¡æ¯ï¼Œè‚¡ç¥¨åç§°å­—æ®µå°†ä¸ºç©º")
    
    # ç™»å½• Baostock
    print("\næ­¥éª¤ 2/4: ç™»å½• Baostock...")
    lg = bs.login()
    if lg.error_code != '0':
        logger.error(f"âŒ ç™»å½•å¤±è´¥: {lg.error_msg}")
        print(f"âŒ ç™»å½•å¤±è´¥: {lg.error_msg}")
        return
    
    logger.info("âœ… ç™»å½•æˆåŠŸ")
    print("âœ… ç™»å½•æˆåŠŸ")
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    print("\næ­¥éª¤ 3/4: è·å–è‚¡ç¥¨åˆ—è¡¨...")
    stock_codes = get_stock_list()
    
    if not stock_codes:
        print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
        bs.logout()
        return
    
    print(f"âœ… è·å–åˆ° {len(stock_codes)} åªè‚¡ç¥¨")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total': len(stock_codes),
        'updated': 0,
        'skipped': 0,
        'failed': 0,
        'new_records': 0
    }
    
    # ä¸‹è½½æ•°æ®
    print(f"\næ­¥éª¤ 4/4: å¢é‡æ›´æ–°Kçº¿æ•°æ®...")
    print(f"æç¤ºï¼šåªä¸‹è½½ç¼ºå¤±çš„äº¤æ˜“æ—¥æ•°æ®ï¼Œå¹¶è‡ªåŠ¨è®¡ç®—æ´¾ç”Ÿå­—æ®µ\n")
    
    with tqdm(total=len(stock_codes), desc="æ›´æ–°è¿›åº¦") as pbar:
        for i, stock_code in enumerate(stock_codes):
            # è¯»å–ç°æœ‰æ•°æ®
            df_existing, latest_date = get_existing_data(stock_code)
            
            # è®¡ç®—ä¸‹è½½èŒƒå›´
            start_date, end_date, need_download = calculate_download_range(latest_date, stock_code)
            
            if not need_download:
                stats['skipped'] += 1
                pbar.update(1)
                continue
            
            # å¤‡ä»½
            if INCREMENTAL_CONFIG['backup_before_update'] and df_existing is not None:
                file_path = OUTPUT_DIR / f"{stock_code}.parquet"
                backup_file(file_path)
            
            # ä¸‹è½½æ–°æ•°æ®
            df_new = download_kline_data(stock_code, start_date, end_date)
            
            if df_new is None:
                stats['failed'] += 1
                pbar.update(1)
                continue
            
            # åˆå¹¶æ•°æ®
            df_final = merge_data(df_existing, df_new, stock_code, stock_info)
            
            if df_final is None or df_final.empty:
                stats['failed'] += 1
                pbar.update(1)
                continue
            
            # éªŒè¯æ¢æ‰‹ç‡å­—æ®µ
            validate_turnover_field(df_final, stock_code)
            
            # ä¿å­˜
            output_file = OUTPUT_DIR / f"{stock_code}.parquet"
            df_final.to_parquet(output_file, index=False)
            
            new_records = len(df_new) if not df_new.empty else 0
            stats['new_records'] += new_records
            stats['updated'] += 1
            
            pbar.update(1)
            
            # æ¯å¤„ç†ä¸€æ‰¹æ˜¾ç¤ºç»Ÿè®¡
            if (i + 1) % BATCH_SIZE == 0:
                pbar.set_postfix({
                    'å·²æ›´æ–°': stats['updated'],
                    'å·²è·³è¿‡': stats['skipped'],
                    'æ–°å¢': stats['new_records']
                })
    
    # é€€å‡ºç™»å½•
    bs.logout()
    logger.info("å·²é€€å‡º Baostock")
    
    # éªŒè¯ç»“æœ
    print(f"\néªŒè¯ç»“æœ...")
    total_files = len(list(OUTPUT_DIR.glob("*.parquet")))
    print(f"âœ… Kçº¿æ–‡ä»¶æ€»æ•°: {total_files}")
    
    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 80)
    print("æ›´æ–°å®Œæˆç»Ÿè®¡")
    print("=" * 80)
    print(f"æ€»è‚¡ç¥¨æ•°: {stats['total']}")
    print(f"âœ… å·²æ›´æ–°: {stats['updated']}")
    print(f"â­ï¸  å·²è·³è¿‡: {stats['skipped']} (æ•°æ®å·²æ˜¯æœ€æ–°)")
    print(f"âŒ æ›´æ–°å¤±è´¥: {stats['failed']}")
    print(f"ğŸ“Š æ–°å¢è®°å½•: {stats['new_records']} æ¡")
    print("=" * 80)
    
    # æ˜¾ç¤ºæ•°æ®ç¤ºä¾‹
    if total_files > 0:
        print(f"\nğŸ“Š æ•°æ®ç¤ºä¾‹ï¼ˆæœ€æ–°5æ¡ï¼‰:")
        sample_file = list(OUTPUT_DIR.glob("*.parquet"))[0]
        sample_df = pd.read_parquet(sample_file)
        print(f"  è‚¡ç¥¨: {sample_file.stem}")
        print(f"  æ•°æ®è¡Œæ•°: {len(sample_df):,}")
        print(f"  æ—¥æœŸèŒƒå›´: {sample_df['æ—¥æœŸ'].min()} è‡³ {sample_df['æ—¥æœŸ'].max()}")
        print(f"  åˆ—å: {list(sample_df.columns)}")
        
        # éªŒè¯å­—æ®µå®Œæ•´æ€§
        required_fields = ['æ¢æ‰‹ç‡', 'æ˜¨æ”¶', 'æ¶¨è·Œé¢', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…å¼‚å¸¸', 'åœç‰Œ']
        print(f"\n  å­—æ®µå®Œæ•´æ€§æ£€æŸ¥:")
        for field in required_fields:
            if field in sample_df.columns:
                valid_count = sample_df[field].notna().sum()
                print(f"    âœ… {field}: å­˜åœ¨ ({valid_count}/{len(sample_df)} = {valid_count/len(sample_df)*100:.1f}%)")
            else:
                print(f"    âŒ {field}: ç¼ºå¤±ï¼")
        
        display_cols = ['æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æ¶¨è·Œé¢', 'æ¶¨è·Œå¹…', 'æŒ¯å¹…', 'æ¢æ‰‹ç‡']
        available_cols = [col for col in display_cols if col in sample_df.columns]
        print(f"\n  æœ€æ–°æ•°æ®é¢„è§ˆ:")
        print(sample_df[available_cols].tail(5).to_string(index=False))
    
    print("\nğŸ‰ ä¸å¤æƒKçº¿æ•°æ®å¢é‡æ›´æ–°å®Œæˆï¼")
    print(f"ğŸ’¡ æ•°æ®ç›®å½•: {OUTPUT_DIR}")
    print(f"ğŸ’¡ å¤‡ä»½ç›®å½•: {BACKUP_DIR}")
    print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {LOG_DIR}/kline_raw_incremental_*.log")
    
    # ä½¿ç”¨å»ºè®®
    print("\n" + "=" * 80)
    print("ä¿®å¤è¯´æ˜")
    print("=" * 80)
    print("""
âœ… å·²ä¿®å¤çš„å­—æ®µï¼š
   - æ¶¨è·Œé¢ï¼šè‡ªåŠ¨è®¡ç®—ï¼ˆæ”¶ç›˜ - æ˜¨æ”¶ï¼‰
   - æŒ¯å¹…ï¼šè‡ªåŠ¨è®¡ç®—ï¼ˆ(æœ€é«˜ - æœ€ä½) / æ˜¨æ”¶ Ã— 100ï¼‰
   - æ˜¨æ”¶ï¼šä»APIè·å– + è‡ªåŠ¨è®¡ç®—
   - æ¶¨è·Œå¹…å¼‚å¸¸ï¼šæ™ºèƒ½åˆ¤æ–­ï¼ˆæ¶¨è·Œå¹… > Â±10%ï¼‰
   - åœç‰Œï¼šæ™ºèƒ½åˆ¤æ–­ï¼ˆæˆäº¤é‡ = 0ï¼‰

ğŸ’¡ ä½¿ç”¨å»ºè®®ï¼š
   1. å®šæ—¶æ›´æ–°ï¼šå»ºè®®æ¯æ—¥æ”¶ç›˜åè¿è¡Œï¼ˆå¦‚ 18:00ï¼‰
   2. å¼ºåˆ¶å…¨é‡ä¸‹è½½ï¼šINCREMENTAL_CONFIG['force_full_download'] = True
   3. è°ƒæ•´å›æº¯å¤©æ•°ï¼šINCREMENTAL_CONFIG['lookback_days'] = N
   
ğŸ“– è¯»å–æ•°æ®ç¤ºä¾‹ï¼š
   import pandas as pd
   df = pd.read_parquet('data/daily_kline_raw/000001.parquet')
   
   # æŸ¥çœ‹å®Œæ•´å­—æ®µ
   print(df.columns.tolist())
   
   # è®¡ç®—çƒ­åº¦å€¼
   volume_ma10 = df['æˆäº¤é‡'].rolling(10).mean()
   turnover_ma10 = df['æ¢æ‰‹ç‡'].rolling(10).mean()
   hotness = (df['æˆäº¤é‡']/volume_ma10)*0.6 + (df['æ¢æ‰‹ç‡']/turnover_ma10)*0.4
    """)

if __name__ == "__main__":
    main()
