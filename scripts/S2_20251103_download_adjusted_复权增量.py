#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç›´æ¥ä¸‹è½½å‰å¤æƒå’Œåå¤æƒæ•°æ® v5.1 - å¢é‡ä¸‹è½½ç‰ˆï¼ˆä¿®å¤ç‰ˆï¼‰

ä¿®å¤å†…å®¹ï¼š
1. âœ… æ·»åŠ å®Œæ•´å­—æ®µï¼šæ¶¨è·Œé¢ã€æŒ¯å¹…ã€æ˜¨æ”¶ã€æ¶¨è·Œå¹…å¼‚å¸¸ã€åœç‰Œ
2. âœ… è‡ªåŠ¨è®¡ç®—æ´¾ç”Ÿå­—æ®µ
3. âœ… å…¼å®¹å†å²æ•°æ®

ä½œè€…ï¼šClaude
ç‰ˆæœ¬ï¼šv5.1
æ—¥æœŸï¼š2025-11-03
"""

import baostock as bs
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
from tqdm import tqdm
import logging
import time
import shutil

# ============================================================
# é…ç½®
# ============================================================

# ç›®å½•é…ç½®
QFQ_DATA_DIR = Path("data/daily_parquet_qfq")  # å‰å¤æƒæ•°æ®
HFQ_DATA_DIR = Path("data/daily_parquet_hfq")  # åå¤æƒæ•°æ®
STOCK_INFO_FILE = Path("data/stock_basic_info.parquet")  # è‚¡ç¥¨ä¿¡æ¯
BACKUP_DIR = Path("data/backups/adjusted_data")
LOG_DIR = Path("logs")

# åˆ›å»ºç›®å½•
QFQ_DATA_DIR.mkdir(parents=True, exist_ok=True)
HFQ_DATA_DIR.mkdir(parents=True, exist_ok=True)
BACKUP_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)

# ä¸‹è½½é…ç½®
DEFAULT_START_DATE = "1990-01-01"  # é¦–æ¬¡ä¸‹è½½çš„å¼€å§‹æ—¥æœŸ
END_DATE = datetime.now().strftime('%Y-%m-%d')  # ç»“æŸæ—¥æœŸï¼ˆä»Šå¤©ï¼‰
MAX_RETRIES = 3
RETRY_DELAY = 2
BATCH_SIZE = 50

# å¢é‡æ›´æ–°é…ç½®
INCREMENTAL_CONFIG = {
    'force_full_download': False,     # æ˜¯å¦å¼ºåˆ¶å…¨é‡ä¸‹è½½
    'lookback_days': 10,              # å›æº¯å¤©æ•°ï¼šé˜²æ­¢æ•°æ®é—æ¼
    'min_gap_days': 1,                # æœ€å°æ›´æ–°é—´éš”ï¼šè·ç¦»æœ€æ–°æ•°æ®<Nå¤©ä¸æ›´æ–°
    'backup_before_update': True,     # æ›´æ–°å‰æ˜¯å¦å¤‡ä»½
}

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(
            LOG_DIR / f'adjusted_incremental_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
            encoding='utf-8'
        ),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================

def add_market_prefix(pure_code):
    """æ·»åŠ å¸‚åœºå‰ç¼€"""
    pure_code = str(pure_code).zfill(6)
    if pure_code.startswith('6') or pure_code.startswith('5'):
        return f'sh.{pure_code}'
    else:
        return f'sz.{pure_code}'

def format_date_string(date_value):
    """æ ¼å¼åŒ–æ—¥æœŸä¸º YYYY-MM-DD å­—ç¬¦ä¸²"""
    if pd.isna(date_value):
        return None
    try:
        if isinstance(date_value, str):
            if len(date_value) == 10 and date_value[4] == '-' and date_value[7] == '-':
                return date_value
        dt = pd.to_datetime(date_value)
        return dt.strftime('%Y-%m-%d')
    except:
        return None

def calculate_derived_fields(df):
    """
    è®¡ç®—æ´¾ç”Ÿå­—æ®µ
    
    æ–°å¢å­—æ®µï¼š
    - æ¶¨è·Œé¢ï¼šæ”¶ç›˜ - æ˜¨æ”¶
    - æŒ¯å¹…ï¼š(æœ€é«˜ - æœ€ä½) / æ˜¨æ”¶ * 100
    - æ˜¨æ”¶ï¼šå‰ä¸€å¤©çš„æ”¶ç›˜ä»·
    """
    if df.empty:
        return df
    
    # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
    df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
    
    # è®¡ç®—æ˜¨æ”¶ï¼ˆå‰ä¸€å¤©çš„æ”¶ç›˜ä»·ï¼‰
    df['æ˜¨æ”¶'] = df['æ”¶ç›˜'].shift(1)
    
    # è®¡ç®—æ¶¨è·Œé¢
    df['æ¶¨è·Œé¢'] = df['æ”¶ç›˜'] - df['æ˜¨æ”¶']
    
    # è®¡ç®—æŒ¯å¹…
    df['æŒ¯å¹…'] = ((df['æœ€é«˜'] - df['æœ€ä½']) / df['æ˜¨æ”¶'] * 100).round(4)
    
    # åˆ¤æ–­æ¶¨è·Œå¹…å¼‚å¸¸ï¼ˆæ¶¨è·Œå¹…è¶…è¿‡Â±10%ï¼Œæˆ–STè‚¡è¶…è¿‡Â±5%ï¼‰
    # ç®€åŒ–ç‰ˆæœ¬ï¼šæ¶¨è·Œå¹… > 10% æˆ– < -10% æ ‡è®°ä¸ºå¼‚å¸¸
    df['æ¶¨è·Œå¹…å¼‚å¸¸'] = df['æ¶¨è·Œå¹…'].apply(
        lambda x: 'X' if (pd.notna(x) and (abs(x) > 10)) else None
    )
    
    # åœç‰Œåˆ¤æ–­ï¼ˆæˆäº¤é‡ä¸º0è§†ä¸ºåœç‰Œï¼‰
    df['åœç‰Œ'] = df['æˆäº¤é‡'].apply(lambda x: 'X' if (pd.notna(x) and x == 0) else None)
    
    return df

def get_existing_data(stock_code, data_dir):
    """
    è¯»å–ç°æœ‰æ•°æ®
    
    è¿”å›: (DataFrame, æœ€æ–°æ—¥æœŸ)
    """
    file_path = data_dir / f"{stock_code}.parquet"
    
    if not file_path.exists():
        return None, None
    
    try:
        df = pd.read_parquet(file_path)
        
        if df.empty:
            return None, None
        
        # è·å–æœ€æ–°æ—¥æœŸ
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        latest_date = df['æ—¥æœŸ'].max().strftime('%Y-%m-%d')
        
        logger.debug(f"{stock_code}: ç°æœ‰æ•°æ® {len(df)} æ¡ï¼Œæœ€æ–°æ—¥æœŸ {latest_date}")
        
        return df, latest_date
    
    except Exception as e:
        logger.error(f"{stock_code}: è¯»å–ç°æœ‰æ•°æ®å¤±è´¥ - {e}")
        return None, None

def calculate_download_range(latest_date, stock_code):
    """
    è®¡ç®—éœ€è¦ä¸‹è½½çš„æ—¥æœŸèŒƒå›´
    
    è¿”å›: (å¼€å§‹æ—¥æœŸ, ç»“æŸæ—¥æœŸ, æ˜¯å¦éœ€è¦ä¸‹è½½)
    """
    if INCREMENTAL_CONFIG['force_full_download']:
        return DEFAULT_START_DATE, END_DATE, True
    
    if latest_date is None:
        # é¦–æ¬¡ä¸‹è½½
        return DEFAULT_START_DATE, END_DATE, True
    
    # è§£ææœ€æ–°æ—¥æœŸ
    latest_dt = datetime.strptime(latest_date, '%Y-%m-%d')
    today = datetime.now()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    days_gap = (today - latest_dt).days
    
    if days_gap <= INCREMENTAL_CONFIG['min_gap_days']:
        logger.debug(f"{stock_code}: æ•°æ®å·²æ˜¯æœ€æ–°ï¼ˆè·ä»Š{days_gap}å¤©ï¼‰ï¼Œè·³è¿‡ä¸‹è½½")
        return None, None, False
    
    # è®¡ç®—ä¸‹è½½èŒƒå›´ï¼ˆå›æº¯Nå¤©é˜²æ­¢é—æ¼ï¼‰
    start_dt = latest_dt - timedelta(days=INCREMENTAL_CONFIG['lookback_days'])
    start_date = start_dt.strftime('%Y-%m-%d')
    
    logger.debug(f"{stock_code}: å¢é‡ä¸‹è½½ {start_date} è‡³ {END_DATE}")
    
    return start_date, END_DATE, True

def download_adjusted_data(stock_code, adjust_type='qfq', start_date=None, end_date=None, retry_count=0):
    """
    ä¸‹è½½å•åªè‚¡ç¥¨çš„å¤æƒæ•°æ®
    
    å‚æ•°:
        stock_code: è‚¡ç¥¨ä»£ç ï¼ˆ6ä½æ•°å­—ï¼‰
        adjust_type: 'qfq' (å‰å¤æƒ) æˆ– 'hfq' (åå¤æƒ)
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        retry_count: å½“å‰é‡è¯•æ¬¡æ•°
    
    è¿”å›: 
        DataFrame æˆ– None
    """
    try:
        # æ·»åŠ å¸‚åœºå‰ç¼€
        bs_code = add_market_prefix(stock_code)
        
        # ä½¿ç”¨ä¼ å…¥çš„æ—¥æœŸèŒƒå›´
        if start_date is None:
            start_date = DEFAULT_START_DATE
        if end_date is None:
            end_date = END_DATE
        
        # è®¾ç½®å¤æƒå‚æ•°
        adjustflag = '2' if adjust_type == 'qfq' else '1'  # 2=å‰å¤æƒ, 1=åå¤æƒ
        
        # è°ƒç”¨ Baostock API - è¯·æ±‚æ‰€æœ‰å¯ç”¨å­—æ®µ
        rs = bs.query_history_k_data_plus(
            code=bs_code,
            fields="date,open,high,low,close,preclose,volume,amount,turn,pctChg,isST",
            start_date=start_date,
            end_date=end_date,
            frequency="d",
            adjustflag=adjustflag
        )
        
        # æ£€æŸ¥è¿”å›çŠ¶æ€
        if rs.error_code != '0':
            if retry_count < MAX_RETRIES:
                logger.debug(f"{stock_code}: ä¸‹è½½å¤±è´¥ï¼Œé‡è¯• {retry_count + 1}/{MAX_RETRIES}")
                time.sleep(RETRY_DELAY)
                return download_adjusted_data(stock_code, adjust_type, start_date, end_date, retry_count + 1)
            else:
                logger.debug(f"{stock_code}: {rs.error_msg}")
                return None
        
        # æ”¶é›†æ•°æ®
        data_list = []
        while (rs.error_code == '0') & rs.next():
            data_list.append(rs.get_row_data())
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
        if not data_list:
            logger.debug(f"{stock_code}: æ— å¤æƒæ•°æ®")
            return pd.DataFrame()  # è¿”å›ç©ºDataFrame
        
        # è½¬æ¢ä¸º DataFrame
        df = pd.DataFrame(data_list, columns=rs.fields)
        
        # é‡å‘½ååˆ—
        column_mapping = {
            'date': 'æ—¥æœŸ',
            'open': 'å¼€ç›˜',
            'high': 'æœ€é«˜',
            'low': 'æœ€ä½',
            'close': 'æ”¶ç›˜',
            'preclose': 'æ˜¨æ”¶',
            'volume': 'æˆäº¤é‡',
            'amount': 'æˆäº¤é¢',
            'turn': 'æ¢æ‰‹ç‡',
            'pctChg': 'æ¶¨è·Œå¹…',
            'isST': 'STæ ‡è®°'
        }
        
        df = df.rename(columns=column_mapping)
        
        # è½¬æ¢æ•°æ®ç±»å‹
        numeric_columns = ['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æ˜¨æ”¶', 'æˆäº¤é¢', 'æ¢æ‰‹ç‡', 'æ¶¨è·Œå¹…']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        if 'æˆäº¤é‡' in df.columns:
            df['æˆäº¤é‡'] = pd.to_numeric(df['æˆäº¤é‡'], errors='coerce').astype('Int64')
        
        # æ ¼å¼åŒ–æ—¥æœŸ
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].apply(format_date_string)
        
        # åˆ é™¤æ— æ•ˆè¡Œ
        df = df.dropna(subset=['æ—¥æœŸ'])
        
        if df.empty:
            logger.debug(f"{stock_code}: è½¬æ¢åæ— æœ‰æ•ˆæ•°æ®")
            return pd.DataFrame()
        
        # è®¡ç®—æ´¾ç”Ÿå­—æ®µ
        df = calculate_derived_fields(df)
        
        return df
    
    except Exception as e:
        if retry_count < MAX_RETRIES:
            logger.debug(f"{stock_code}: å¼‚å¸¸ï¼Œé‡è¯• {retry_count + 1}/{MAX_RETRIES} - {e}")
            time.sleep(RETRY_DELAY)
            return download_adjusted_data(stock_code, adjust_type, start_date, end_date, retry_count + 1)
        else:
            logger.error(f"{stock_code}: ä¸‹è½½å¼‚å¸¸ - {e}")
            return None

def merge_data(df_existing, df_new, stock_code, stock_info):
    """
    åˆå¹¶ç°æœ‰æ•°æ®å’Œæ–°ä¸‹è½½çš„æ•°æ®
    
    ç­–ç•¥ï¼š
    1. åˆå¹¶ä¸¤ä¸ªDataFrame
    2. æŒ‰æ—¥æœŸå»é‡ï¼ˆä¿ç•™æ–°æ•°æ®ï¼‰
    3. é‡æ–°è®¡ç®—æ‰€æœ‰æ´¾ç”Ÿå­—æ®µï¼ˆç¡®ä¿æ•°æ®ä¸€è‡´æ€§ï¼‰
    4. æ’åº
    5. æ·»åŠ è‚¡ç¥¨ä¿¡æ¯
    """
    if df_existing is None or df_existing.empty:
        df_result = df_new
    elif df_new is None or df_new.empty:
        return df_existing
    else:
        # ç¡®ä¿æ—¥æœŸæ ¼å¼ä¸€è‡´
        df_existing['æ—¥æœŸ'] = pd.to_datetime(df_existing['æ—¥æœŸ'])
        df_new['æ—¥æœŸ'] = pd.to_datetime(df_new['æ—¥æœŸ'])
        
        # åˆå¹¶
        df_result = pd.concat([df_existing, df_new], ignore_index=True)
        
        # å»é‡ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
        df_result = df_result.drop_duplicates(subset=['æ—¥æœŸ'], keep='last')
        
        # æ’åº
        df_result = df_result.sort_values('æ—¥æœŸ').reset_index(drop=True)
    
    # è½¬æ¢æ—¥æœŸå›å­—ç¬¦ä¸²
    df_result['æ—¥æœŸ'] = df_result['æ—¥æœŸ'].dt.strftime('%Y-%m-%d')
    
    # é‡æ–°è®¡ç®—æ´¾ç”Ÿå­—æ®µï¼ˆç¡®ä¿å®Œæ•´æ€§ï¼‰
    df_result = calculate_derived_fields(df_result)
    
    # æ·»åŠ è‚¡ç¥¨ä»£ç 
    df_result['è‚¡ç¥¨ä»£ç '] = stock_code
    
    # æ·»åŠ è‚¡ç¥¨åç§°
    if stock_info is not None:
        stock_row = stock_info[stock_info['è‚¡ç¥¨ä»£ç '] == stock_code]
        if not stock_row.empty:
            df_result['è‚¡ç¥¨åç§°'] = stock_row['è‚¡ç¥¨åç§°'].iloc[0]
    
    return df_result

def get_stock_list():
    """è·å–éœ€è¦å¤„ç†çš„è‚¡ç¥¨åˆ—è¡¨"""
    # ä¼˜å…ˆä»å·²æœ‰æ•°æ®ä¸­è·å–
    stock_codes = set()
    
    # ä»å‰å¤æƒç›®å½•è·å–
    if QFQ_DATA_DIR.exists():
        qfq_files = list(QFQ_DATA_DIR.glob("*.parquet"))
        stock_codes.update([f.stem for f in qfq_files])
    
    # ä»åå¤æƒç›®å½•è·å–
    if HFQ_DATA_DIR.exists():
        hfq_files = list(HFQ_DATA_DIR.glob("*.parquet"))
        stock_codes.update([f.stem for f in hfq_files])
    
    # å¦‚æœæ²¡æœ‰å·²æœ‰æ•°æ®ï¼Œä»è‚¡ç¥¨ä¿¡æ¯æ–‡ä»¶è·å–
    if not stock_codes and STOCK_INFO_FILE.exists():
        try:
            stock_info = pd.read_parquet(STOCK_INFO_FILE)
            stock_codes = set(stock_info['è‚¡ç¥¨ä»£ç '].astype(str).tolist())
        except Exception as e:
            logger.error(f"è¯»å–è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {e}")
    
    stock_codes = sorted(list(stock_codes))
    logger.info(f"âœ… è·å–åˆ° {len(stock_codes)} åªè‚¡ç¥¨")
    
    return stock_codes

def load_stock_info():
    """åŠ è½½è‚¡ç¥¨ä¿¡æ¯"""
    try:
        if not STOCK_INFO_FILE.exists():
            logger.warning(f"âš ï¸  è‚¡ç¥¨ä¿¡æ¯æ–‡ä»¶ä¸å­˜åœ¨: {STOCK_INFO_FILE}")
            return None
        
        logger.info(f"åŠ è½½è‚¡ç¥¨ä¿¡æ¯: {STOCK_INFO_FILE}")
        stock_info = pd.read_parquet(STOCK_INFO_FILE)
        
        # ç¡®ä¿è‚¡ç¥¨ä»£ç æ˜¯å­—ç¬¦ä¸²æ ¼å¼
        if 'è‚¡ç¥¨ä»£ç ' in stock_info.columns:
            stock_info['è‚¡ç¥¨ä»£ç '] = stock_info['è‚¡ç¥¨ä»£ç '].astype(str).str.zfill(6)
        
        logger.info(f"âœ… è‚¡ç¥¨ä¿¡æ¯æ•°æ®: {len(stock_info)} åªè‚¡ç¥¨")
        
        return stock_info
    except Exception as e:
        logger.error(f"âŒ åŠ è½½è‚¡ç¥¨ä¿¡æ¯å¤±è´¥: {e}")
        return None

def backup_file(file_path):
    """å¤‡ä»½å•ä¸ªæ–‡ä»¶"""
    if not file_path.exists():
        return
    
    try:
        # åˆ›å»ºè‚¡ç¥¨ä»£ç å¯¹åº”çš„å¤‡ä»½ç›®å½•
        stock_code = file_path.stem
        backup_subdir = BACKUP_DIR / stock_code
        backup_subdir.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = backup_subdir / f"{stock_code}_{timestamp}.parquet"
        
        # å¤åˆ¶æ–‡ä»¶
        shutil.copy2(file_path, backup_file)
        logger.debug(f"å·²å¤‡ä»½: {backup_file}")
        
        # æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘3ä¸ªï¼‰
        backups = sorted(backup_subdir.glob(f"{stock_code}_*.parquet"), reverse=True)
        for old_backup in backups[3:]:
            old_backup.unlink()
            logger.debug(f"åˆ é™¤æ—§å¤‡ä»½: {old_backup}")
    
    except Exception as e:
        logger.warning(f"å¤‡ä»½å¤±è´¥: {e}")

# ============================================================
# ä¸»ç¨‹åº
# ============================================================

def main():
    print("=" * 80)
    print("  ä¸‹è½½å‰å¤æƒå’Œåå¤æƒæ•°æ® v5.1 - å¢é‡ä¸‹è½½ç‰ˆï¼ˆä¿®å¤ç‰ˆï¼‰")
    print("  æ™ºèƒ½æ£€æµ‹ + å¢é‡æ›´æ–° + å®Œæ•´å­—æ®µ + æ•°æ®éªŒè¯")
    print("=" * 80)
    
    print(f"\né…ç½®:")
    print(f"  å¼ºåˆ¶å…¨é‡ä¸‹è½½: {'æ˜¯' if INCREMENTAL_CONFIG['force_full_download'] else 'å¦'}")
    print(f"  å›æº¯å¤©æ•°: {INCREMENTAL_CONFIG['lookback_days']} å¤©")
    print(f"  æœ€å°æ›´æ–°é—´éš”: {INCREMENTAL_CONFIG['min_gap_days']} å¤©")
    print(f"  ç»“æŸæ—¥æœŸ: {END_DATE}")
    print(f"  æ›´æ–°å‰å¤‡ä»½: {'æ˜¯' if INCREMENTAL_CONFIG['backup_before_update'] else 'å¦'}")
    
    print(f"\nâœ¨ æ–°å¢åŠŸèƒ½:")
    print(f"  âœ… è‡ªåŠ¨è®¡ç®—ï¼šæ¶¨è·Œé¢ã€æŒ¯å¹…ã€æ˜¨æ”¶")
    print(f"  âœ… æ™ºèƒ½åˆ¤æ–­ï¼šæ¶¨è·Œå¹…å¼‚å¸¸ã€åœç‰ŒçŠ¶æ€")
    
    # è·å–è‚¡ç¥¨åˆ—è¡¨
    print("\næ­¥éª¤ 1/5: è·å–è‚¡ç¥¨åˆ—è¡¨...")
    stock_codes = get_stock_list()
    
    if not stock_codes:
        print("âŒ æ— æ³•è·å–è‚¡ç¥¨åˆ—è¡¨")
        print("æç¤º: è¯·ç¡®ä¿å·²æœ‰æ•°æ®æˆ–è‚¡ç¥¨ä¿¡æ¯æ–‡ä»¶å­˜åœ¨")
        return
    
    print(f"âœ… è·å–åˆ° {len(stock_codes)} åªè‚¡ç¥¨")
    
    # åŠ è½½è‚¡ç¥¨ä¿¡æ¯
    print("\næ­¥éª¤ 2/5: åŠ è½½è‚¡ç¥¨ä¿¡æ¯...")
    stock_info = load_stock_info()
    if stock_info is not None:
        print(f"âœ… è‚¡ç¥¨ä¿¡æ¯åŠ è½½æˆåŠŸ: {len(stock_info)} åªè‚¡ç¥¨")
    else:
        print(f"âš ï¸  æœªåŠ è½½è‚¡ç¥¨ä¿¡æ¯ï¼Œè‚¡ç¥¨åç§°å­—æ®µå°†ä¸ºç©º")
    
    # ç™»å½• Baostock
    print("\næ­¥éª¤ 3/5: ç™»å½• Baostock...")
    lg = bs.login()
    if lg.error_code != '0':
        logger.error(f"âŒ ç™»å½•å¤±è´¥: {lg.error_msg}")
        print(f"âŒ ç™»å½•å¤±è´¥: {lg.error_msg}")
        return
    
    logger.info("âœ… ç™»å½•æˆåŠŸ")
    print("âœ… ç™»å½•æˆåŠŸ")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total': len(stock_codes),
        'qfq_updated': 0,
        'qfq_skipped': 0,
        'qfq_failed': 0,
        'hfq_updated': 0,
        'hfq_skipped': 0,
        'hfq_failed': 0,
        'new_records': 0
    }
    
    # ä¸‹è½½æ•°æ®
    print(f"\næ­¥éª¤ 4/5: å¢é‡æ›´æ–°å¤æƒæ•°æ®...")
    print(f"æç¤ºï¼šåªä¸‹è½½ç¼ºå¤±çš„äº¤æ˜“æ—¥æ•°æ®ï¼Œå¹¶è‡ªåŠ¨è®¡ç®—æ´¾ç”Ÿå­—æ®µ\n")
    
    with tqdm(total=len(stock_codes), desc="æ›´æ–°è¿›åº¦") as pbar:
        for i, stock_code in enumerate(stock_codes):
            # === å¤„ç†å‰å¤æƒæ•°æ® ===
            df_qfq_existing, latest_qfq_date = get_existing_data(stock_code, QFQ_DATA_DIR)
            start_date, end_date, need_download = calculate_download_range(latest_qfq_date, stock_code)
            
            if need_download:
                # å¤‡ä»½
                if INCREMENTAL_CONFIG['backup_before_update'] and df_qfq_existing is not None:
                    qfq_file = QFQ_DATA_DIR / f"{stock_code}.parquet"
                    backup_file(qfq_file)
                
                # ä¸‹è½½æ–°æ•°æ®
                df_qfq_new = download_adjusted_data(stock_code, 'qfq', start_date, end_date)
                
                if df_qfq_new is not None:
                    # åˆå¹¶æ•°æ®
                    df_qfq_final = merge_data(df_qfq_existing, df_qfq_new, stock_code, stock_info)
                    
                    # ä¿å­˜
                    output_file = QFQ_DATA_DIR / f"{stock_code}.parquet"
                    df_qfq_final.to_parquet(output_file, index=False)
                    
                    new_records = len(df_qfq_new) if not df_qfq_new.empty else 0
                    stats['new_records'] += new_records
                    stats['qfq_updated'] += 1
                else:
                    stats['qfq_failed'] += 1
            else:
                stats['qfq_skipped'] += 1
            
            # === å¤„ç†åå¤æƒæ•°æ® ===
            df_hfq_existing, latest_hfq_date = get_existing_data(stock_code, HFQ_DATA_DIR)
            start_date, end_date, need_download = calculate_download_range(latest_hfq_date, stock_code)
            
            if need_download:
                # å¤‡ä»½
                if INCREMENTAL_CONFIG['backup_before_update'] and df_hfq_existing is not None:
                    hfq_file = HFQ_DATA_DIR / f"{stock_code}.parquet"
                    backup_file(hfq_file)
                
                # ä¸‹è½½æ–°æ•°æ®
                df_hfq_new = download_adjusted_data(stock_code, 'hfq', start_date, end_date)
                
                if df_hfq_new is not None:
                    # åˆå¹¶æ•°æ®
                    df_hfq_final = merge_data(df_hfq_existing, df_hfq_new, stock_code, stock_info)
                    
                    # ä¿å­˜
                    output_file = HFQ_DATA_DIR / f"{stock_code}.parquet"
                    df_hfq_final.to_parquet(output_file, index=False)
                    
                    stats['hfq_updated'] += 1
                else:
                    stats['hfq_failed'] += 1
            else:
                stats['hfq_skipped'] += 1
            
            pbar.update(1)
            
            # æ¯å¤„ç†ä¸€æ‰¹æ˜¾ç¤ºç»Ÿè®¡
            if (i + 1) % BATCH_SIZE == 0:
                pbar.set_postfix({
                    'QFQæ›´æ–°': stats['qfq_updated'],
                    'HFQæ›´æ–°': stats['hfq_updated'],
                    'æ–°å¢': stats['new_records']
                })
    
    # é€€å‡ºç™»å½•
    bs.logout()
    logger.info("å·²é€€å‡º Baostock")
    
    # éªŒè¯ç»“æœ
    print(f"\næ­¥éª¤ 5/5: éªŒè¯ç»“æœ...")
    qfq_count = len(list(QFQ_DATA_DIR.glob("*.parquet")))
    hfq_count = len(list(HFQ_DATA_DIR.glob("*.parquet")))
    
    print(f"âœ… å‰å¤æƒæ–‡ä»¶æ•°: {qfq_count}")
    print(f"âœ… åå¤æƒæ–‡ä»¶æ•°: {hfq_count}")
    
    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 80)
    print("æ›´æ–°å®Œæˆç»Ÿè®¡")
    print("=" * 80)
    print(f"æ€»è‚¡ç¥¨æ•°: {stats['total']}")
    print(f"\nå‰å¤æƒæ•°æ®:")
    print(f"  âœ… å·²æ›´æ–°: {stats['qfq_updated']}")
    print(f"  â­ï¸  å·²è·³è¿‡: {stats['qfq_skipped']} (æ•°æ®å·²æ˜¯æœ€æ–°)")
    print(f"  âŒ æ›´æ–°å¤±è´¥: {stats['qfq_failed']}")
    print(f"\nåå¤æƒæ•°æ®:")
    print(f"  âœ… å·²æ›´æ–°: {stats['hfq_updated']}")
    print(f"  â­ï¸  å·²è·³è¿‡: {stats['hfq_skipped']} (æ•°æ®å·²æ˜¯æœ€æ–°)")
    print(f"  âŒ æ›´æ–°å¤±è´¥: {stats['hfq_failed']}")
    print(f"\nğŸ“Š æ–°å¢è®°å½•: {stats['new_records']} æ¡")
    print("=" * 80)
    
    # æ˜¾ç¤ºæ•°æ®ç¤ºä¾‹
    if qfq_count > 0:
        print(f"\nğŸ“Š æ•°æ®ç¤ºä¾‹ï¼ˆå‰å¤æƒï¼Œæœ€æ–°5æ¡ï¼‰:")
        sample_file = list(QFQ_DATA_DIR.glob("*.parquet"))[0]
        sample_df = pd.read_parquet(sample_file)
        print(f"  è‚¡ç¥¨: {sample_file.stem}")
        print(f"  æ•°æ®è¡Œæ•°: {len(sample_df):,}")
        print(f"  æ—¥æœŸèŒƒå›´: {sample_df['æ—¥æœŸ'].min()} è‡³ {sample_df['æ—¥æœŸ'].max()}")
        
        display_cols = ['æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æ¶¨è·Œé¢', 'æ¶¨è·Œå¹…', 'æŒ¯å¹…', 'æ˜¨æ”¶']
        available_cols = [col for col in display_cols if col in sample_df.columns]
        print(sample_df[available_cols].tail(5).to_string(index=False))
    
    print("\nğŸ‰ å¤æƒæ•°æ®å¢é‡æ›´æ–°å®Œæˆï¼")
    print(f"ğŸ’¡ æ•°æ®ç›®å½•: {QFQ_DATA_DIR} å’Œ {HFQ_DATA_DIR}")
    print(f"ğŸ’¡ å¤‡ä»½ç›®å½•: {BACKUP_DIR}")
    print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {LOG_DIR}/adjusted_incremental_*.log")
    
    # ä½¿ç”¨å»ºè®®
    print("\n" + "=" * 80)
    print("ä¿®å¤è¯´æ˜")
    print("=" * 80)
    print("""
âœ… å·²ä¿®å¤çš„å­—æ®µï¼š
   - æ¶¨è·Œé¢ï¼šè‡ªåŠ¨è®¡ç®—ï¼ˆæ”¶ç›˜ - æ˜¨æ”¶ï¼‰
   - æŒ¯å¹…ï¼šè‡ªåŠ¨è®¡ç®—ï¼ˆ(æœ€é«˜ - æœ€ä½) / æ˜¨æ”¶ Ã— 100ï¼‰
   - æ˜¨æ”¶ï¼šä»APIè·å– + è‡ªåŠ¨è®¡ç®—
   - æ¶¨è·Œå¹…å¼‚å¸¸ï¼šæ™ºèƒ½åˆ¤æ–­ï¼ˆæ¶¨è·Œå¹… > Â±10%ï¼‰
   - åœç‰Œï¼šæ™ºèƒ½åˆ¤æ–­ï¼ˆæˆäº¤é‡ = 0ï¼‰

ğŸ’¡ æ³¨æ„äº‹é¡¹ï¼š
   1. è¿è¡Œåä¼šè‡ªåŠ¨è¡¥å…¨æ‰€æœ‰å†å²æ•°æ®çš„æ´¾ç”Ÿå­—æ®µ
   2. ç¬¬ä¸€è¡Œæ•°æ®çš„"æ˜¨æ”¶"ã€"æ¶¨è·Œé¢"ã€"æŒ¯å¹…"ä¼šä¸ºç©ºï¼ˆæ­£å¸¸ï¼‰
   3. 10æœˆ17æ—¥ä¹‹å‰çš„æ•°æ®ä¹Ÿä¼šè¢«é‡æ–°è®¡ç®—ï¼Œç¡®ä¿ä¸€è‡´æ€§
    """)

if __name__ == "__main__":
    main()
