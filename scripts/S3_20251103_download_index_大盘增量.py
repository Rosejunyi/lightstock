#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸‹è½½å¤§ç›˜æŒ‡æ•°æ•°æ® v2.1 - å¢é‡ä¸‹è½½ç‰ˆï¼ˆä¿®å¤ç‰ˆï¼‰

ä¿®å¤å†…å®¹ï¼š
1. âœ… ä¿®å¤æ—¥æœŸç±»å‹è½¬æ¢é”™è¯¯ï¼ˆç¬¬352è¡Œï¼‰
2. âœ… å¢å¼ºæ™ºèƒ½è·³è¿‡ï¼šè€ƒè™‘äº¤æ˜“æ—¥è€Œéè‡ªç„¶æ—¥
3. âœ… æ·»åŠ æ•°æ®æ—¥æœŸæ£€æŸ¥ï¼šé¿å…é‡å¤ä¸‹è½½

ä½œè€…ï¼šClaude
ç‰ˆæœ¬ï¼šv2.1
æ—¥æœŸï¼š2025-11-10
"""

import baostock as bs
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
from tqdm import tqdm
import logging
import shutil

# ============================================================
# é…ç½®
# ============================================================

# å¤§ç›˜æŒ‡æ•°é…ç½®
INDICES = {
    'sh.000001': {'name': 'ä¸Šè¯æŒ‡æ•°', 'code': '999999', 'download_code': 'sh.000001'},
    'sz.399001': {'name': 'æ·±è¯æˆæŒ‡', 'code': '399001', 'download_code': 'sz.399001'},
    'sh.000300': {'name': 'æ²ªæ·±300', 'code': '000300', 'download_code': 'sh.000300'},
    'sz.399006': {'name': 'åˆ›ä¸šæ¿æŒ‡', 'code': '399006', 'download_code': 'sz.399006'},
}

# ç›®å½•é…ç½®
OUTPUT_DIR = Path("data/index_data")
BACKUP_DIR = Path("data/backups/index_data")
LOG_DIR = Path("logs")

# ä¸‹è½½é…ç½®
DEFAULT_START_DATE = "1990-01-01"
END_DATE = datetime.now().strftime('%Y-%m-%d')
MAX_RETRIES = 3
RETRY_DELAY = 2

# å¢é‡æ›´æ–°é…ç½® - ä¼˜åŒ–ç‰ˆ
INCREMENTAL_CONFIG = {
    'force_full_download': False,
    'lookback_days': 10,
    'min_gap_days': 1,  # è·ç¦»æœ€æ–°æ•°æ®<Nå¤©ä¸æ›´æ–°
    'backup_before_update': True,
    'smart_trading_day_check': True,  # âœ¨ æ–°å¢ï¼šæ™ºèƒ½äº¤æ˜“æ—¥æ£€æµ‹
}

# åˆ›å»ºç›®å½•
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
BACKUP_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(
            LOG_DIR / f'index_incremental_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
            encoding='utf-8'
        ),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================

def is_trading_day_today():
    """
    åˆ¤æ–­ä»Šå¤©æ˜¯å¦æ˜¯äº¤æ˜“æ—¥
    
    ç®€å•è§„åˆ™ï¼š
    - å‘¨å…­æ—¥ï¼šéäº¤æ˜“æ—¥
    - å·¥ä½œæ—¥ï¼šå¯èƒ½æ˜¯äº¤æ˜“æ—¥ï¼ˆä¸è€ƒè™‘èŠ‚å‡æ—¥ï¼‰
    
    è¿”å›: (æ˜¯å¦æ˜¯äº¤æ˜“æ—¥, è·ç¦»ä¸Šä¸ªäº¤æ˜“æ—¥çš„è‡ªç„¶æ—¥å¤©æ•°)
    """
    today = datetime.now()
    weekday = today.weekday()  # 0=å‘¨ä¸€, 6=å‘¨æ—¥
    
    if weekday >= 5:  # å‘¨å…­æˆ–å‘¨æ—¥
        # è®¡ç®—è·ç¦»ä¸Šä¸ªå‘¨äº”çš„å¤©æ•°
        days_since_friday = weekday - 4
        return False, days_since_friday
    
    # å·¥ä½œæ—¥ï¼Œå‡è®¾æ˜¯äº¤æ˜“æ—¥
    return True, 0

def format_date_string(date_value):
    """æ ¼å¼åŒ–æ—¥æœŸä¸º YYYY-MM-DD å­—ç¬¦ä¸²"""
    if pd.isna(date_value):
        return None
    try:
        if isinstance(date_value, str):
            if len(date_value) == 10 and date_value[4] == '-' and date_value[7] == '-':
                return date_value
        dt = pd.to_datetime(date_value)
        return dt.strftime('%Y-%m-%d')
    except:
        return None

def calculate_derived_fields(df):
    """
    è®¡ç®—æ´¾ç”Ÿå­—æ®µ
    
    æ–°å¢å­—æ®µï¼š
    - æ¶¨è·Œé¢ï¼šæ”¶ç›˜ - æ˜¨æ”¶
    - æŒ¯å¹…ï¼š(æœ€é«˜ - æœ€ä½) / æ˜¨æ”¶ * 100
    - æ˜¨æ”¶ï¼šå‰ä¸€å¤©çš„æ”¶ç›˜ä»·
    - æ³¢åŠ¨å¼‚å¸¸ï¼šå•æ—¥æ¶¨è·Œå¹…è¶…è¿‡Â±5%
    """
    if df.empty:
        return df
    
    # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
    df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
    
    # å¦‚æœAPIæ²¡æœ‰æä¾›æ˜¨æ”¶ï¼Œä½¿ç”¨å‰ä¸€å¤©çš„æ”¶ç›˜ä»·è®¡ç®—
    if 'æ˜¨æ”¶' not in df.columns or df['æ˜¨æ”¶'].isna().all():
        df['æ˜¨æ”¶'] = df['æ”¶ç›˜'].shift(1)
    
    # è®¡ç®—æ¶¨è·Œé¢
    df['æ¶¨è·Œé¢'] = df['æ”¶ç›˜'] - df['æ˜¨æ”¶']
    
    # è®¡ç®—æŒ¯å¹…
    df['æŒ¯å¹…'] = ((df['æœ€é«˜'] - df['æœ€ä½']) / df['æ˜¨æ”¶'] * 100).round(4)
    
    # åˆ¤æ–­æ³¢åŠ¨å¼‚å¸¸
    df['æ³¢åŠ¨å¼‚å¸¸'] = df['æ¶¨è·Œå¹…'].apply(
        lambda x: 'X' if (pd.notna(x) and (abs(x) > 5)) else None
    )
    
    return df

def calculate_period_returns(df):
    """è®¡ç®—ä¸åŒå‘¨æœŸçš„æ”¶ç›Šç‡"""
    df = df.copy()
    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
    df = df.sort_values('æ—¥æœŸ')
    
    df['æ¶¨è·Œå¹…_1æœˆ'] = df['æ”¶ç›˜'].pct_change(22) * 100
    df['æ¶¨è·Œå¹…_3æœˆ'] = df['æ”¶ç›˜'].pct_change(66) * 100
    df['æ¶¨è·Œå¹…_6æœˆ'] = df['æ”¶ç›˜'].pct_change(132) * 100
    
    # è½¬æ¢å›å­—ç¬¦ä¸²æ—¥æœŸ
    df['æ—¥æœŸ'] = df['æ—¥æœŸ'].dt.strftime('%Y-%m-%d')
    
    return df

def get_existing_data(index_code):
    """
    è¯»å–ç°æœ‰æ•°æ®
    
    è¿”å›: (DataFrame, æœ€æ–°æ—¥æœŸ)
    """
    file_path = OUTPUT_DIR / f"{index_code}.parquet"
    
    if not file_path.exists():
        return None, None
    
    try:
        df = pd.read_parquet(file_path)
        
        if df.empty:
            return None, None
        
        # è·å–æœ€æ–°æ—¥æœŸ
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        latest_date = df['æ—¥æœŸ'].max().strftime('%Y-%m-%d')
        
        logger.debug(f"{index_code}: ç°æœ‰æ•°æ® {len(df)} æ¡ï¼Œæœ€æ–°æ—¥æœŸ {latest_date}")
        
        return df, latest_date
    
    except Exception as e:
        logger.error(f"{index_code}: è¯»å–ç°æœ‰æ•°æ®å¤±è´¥ - {e}")
        return None, None

def calculate_download_range(latest_date, index_name):
    """
    è®¡ç®—éœ€è¦ä¸‹è½½çš„æ—¥æœŸèŒƒå›´ï¼ˆæ™ºèƒ½ç‰ˆï¼‰
    
    è¿”å›: (å¼€å§‹æ—¥æœŸ, ç»“æŸæ—¥æœŸ, æ˜¯å¦éœ€è¦ä¸‹è½½)
    """
    if INCREMENTAL_CONFIG['force_full_download']:
        return DEFAULT_START_DATE, END_DATE, True
    
    if latest_date is None:
        # é¦–æ¬¡ä¸‹è½½
        return DEFAULT_START_DATE, END_DATE, True
    
    # è§£ææœ€æ–°æ—¥æœŸ
    latest_dt = datetime.strptime(latest_date, '%Y-%m-%d')
    today = datetime.now()
    
    # è®¡ç®—è‡ªç„¶æ—¥é—´éš”
    days_gap = (today - latest_dt).days
    
    # æ™ºèƒ½äº¤æ˜“æ—¥æ£€æµ‹
    if INCREMENTAL_CONFIG['smart_trading_day_check']:
        is_trading, days_since_last_trading = is_trading_day_today()
        
        # å¦‚æœä»Šå¤©ä¸æ˜¯äº¤æ˜“æ—¥ï¼Œè°ƒæ•´é¢„æœŸé—´éš”
        if not is_trading:
            # ä¾‹å¦‚ï¼šå‘¨å…­è¿è¡Œï¼Œä¸Šä¸ªäº¤æ˜“æ—¥æ˜¯å‘¨äº”ï¼Œæ•°æ®åº”è¯¥æ˜¯å‘¨äº”çš„
            # é‚£ä¹ˆå®é™…é—´éš”åº”è¯¥æ˜¯1å¤©ï¼ˆå‘¨äº”åˆ°å‘¨å…­ï¼‰
            # å¦‚æœæ•°æ®æœ€æ–°æ—¥æœŸæ˜¯å‘¨äº”ï¼Œåˆ™ä¸éœ€è¦æ›´æ–°
            if days_gap <= (1 + days_since_last_trading):
                logger.info(f"â­ï¸  {index_name}: ä»Šå¤©éäº¤æ˜“æ—¥ï¼Œæ•°æ®å·²æ˜¯æœ€æ–°ï¼ˆæœ€æ–°ï¼š{latest_date}ï¼Œè·ä»Š{days_gap}å¤©ï¼‰")
                return None, None, False
        else:
            # ä»Šå¤©æ˜¯äº¤æ˜“æ—¥
            if days_gap <= INCREMENTAL_CONFIG['min_gap_days']:
                logger.info(f"â­ï¸  {index_name}: æ•°æ®å·²æ˜¯æœ€æ–°ï¼ˆæœ€æ–°ï¼š{latest_date}ï¼Œè·ä»Š{days_gap}å¤©ï¼‰")
                return None, None, False
    else:
        # ç®€å•æ£€æŸ¥
        if days_gap <= INCREMENTAL_CONFIG['min_gap_days']:
            logger.debug(f"{index_name}: æ•°æ®å·²æ˜¯æœ€æ–°ï¼ˆè·ä»Š{days_gap}å¤©ï¼‰ï¼Œè·³è¿‡ä¸‹è½½")
            return None, None, False
    
    # è®¡ç®—ä¸‹è½½èŒƒå›´ï¼ˆå›æº¯Nå¤©é˜²æ­¢é—æ¼ï¼‰
    start_dt = latest_dt - timedelta(days=INCREMENTAL_CONFIG['lookback_days'])
    start_date = start_dt.strftime('%Y-%m-%d')
    
    logger.info(f"âœ… {index_name}: éœ€è¦æ›´æ–°ï¼ˆæœ€æ–°ï¼š{latest_date} â†’ ä»Šå¤©ï¼š{END_DATE}ï¼‰ï¼Œå¢é‡ä¸‹è½½ {start_date} è‡³ {END_DATE}")
    
    return start_date, END_DATE, True

def download_index_data(download_code, index_name, save_code, start_date, end_date, retry_count=0):
    """
    ä¸‹è½½å•ä¸ªæŒ‡æ•°çš„å†å²æ•°æ®
    """
    try:
        logger.info(f"ä¸‹è½½ {index_name} ({start_date} è‡³ {end_date})")
        
        rs = bs.query_history_k_data_plus(
            code=download_code,
            fields="date,open,high,low,close,preclose,volume,amount,pctChg",
            start_date=start_date,
            end_date=end_date,
            frequency="d",
            adjustflag="3"
        )
        
        if rs.error_code != '0':
            if retry_count < MAX_RETRIES:
                logger.warning(f"{index_name}: ä¸‹è½½å¤±è´¥ï¼Œé‡è¯• {retry_count + 1}/{MAX_RETRIES}")
                import time
                time.sleep(RETRY_DELAY)
                return download_index_data(download_code, index_name, save_code, start_date, end_date, retry_count + 1)
            else:
                logger.error(f"{index_name}: {rs.error_msg}")
                return None
        
        data_list = []
        while (rs.error_code == '0') & rs.next():
            data_list.append(rs.get_row_data())
        
        if not data_list:
            logger.warning(f"{index_name}: æ— æ•°æ®")
            return pd.DataFrame()
        
        df = pd.DataFrame(data_list, columns=rs.fields)
        
        column_mapping = {
            'date': 'æ—¥æœŸ',
            'open': 'å¼€ç›˜',
            'high': 'æœ€é«˜',
            'low': 'æœ€ä½',
            'close': 'æ”¶ç›˜',
            'preclose': 'æ˜¨æ”¶',
            'volume': 'æˆäº¤é‡',
            'amount': 'æˆäº¤é¢',
            'pctChg': 'æ¶¨è·Œå¹…'
        }
        
        df = df.rename(columns=column_mapping)
        
        # è½¬æ¢æ•°å€¼åˆ—
        numeric_cols = ['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æ˜¨æ”¶', 'æˆäº¤é‡', 'æˆäº¤é¢', 'æ¶¨è·Œå¹…']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # æ·»åŠ æŒ‡æ•°æ ‡è¯†
        df['æŒ‡æ•°ä»£ç '] = save_code
        df['æŒ‡æ•°åç§°'] = index_name
        
        logger.info(f"{index_name}: ä¸‹è½½æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•")
        
        return df
    
    except Exception as e:
        logger.error(f"{index_name}: ä¸‹è½½å¼‚å¸¸ - {e}")
        return None

def merge_data(df_existing, df_new, index_name):
    """
    åˆå¹¶æ–°æ—§æ•°æ®ï¼ˆä¿®å¤ç‰ˆï¼‰
    
    âœ… ä¿®å¤ï¼šç¡®ä¿æ—¥æœŸåˆ—åœ¨ä½¿ç”¨.dtä¹‹å‰æ˜¯datetimeç±»å‹
    """
    if df_existing is None or df_existing.empty:
        df_result = df_new
    elif df_new is None or df_new.empty:
        return df_existing
    else:
        # ç¡®ä¿æ—¥æœŸæ ¼å¼ä¸€è‡´
        df_existing['æ—¥æœŸ'] = pd.to_datetime(df_existing['æ—¥æœŸ'])
        df_new['æ—¥æœŸ'] = pd.to_datetime(df_new['æ—¥æœŸ'])
        
        # åˆå¹¶
        df_result = pd.concat([df_existing, df_new], ignore_index=True)
        
        # å»é‡ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
        df_result = df_result.drop_duplicates(subset=['æ—¥æœŸ'], keep='last')
        
        # æ’åº
        df_result = df_result.sort_values('æ—¥æœŸ').reset_index(drop=True)
    
    # âœ… ä¿®å¤ï¼šåœ¨ä½¿ç”¨.dtä¹‹å‰ç¡®ä¿æ—¥æœŸåˆ—æ˜¯datetimeç±»å‹
    if df_result['æ—¥æœŸ'].dtype != 'datetime64[ns]':
        df_result['æ—¥æœŸ'] = pd.to_datetime(df_result['æ—¥æœŸ'])
    
    # è½¬æ¢æ—¥æœŸå›å­—ç¬¦ä¸²
    df_result['æ—¥æœŸ'] = df_result['æ—¥æœŸ'].dt.strftime('%Y-%m-%d')
    
    # é‡æ–°è®¡ç®—æ´¾ç”Ÿå­—æ®µ
    df_result = calculate_derived_fields(df_result)
    
    # é‡æ–°è®¡ç®—å‘¨æœŸæ”¶ç›Šç‡
    df_result = calculate_period_returns(df_result)
    
    logger.info(f"{index_name}: åˆå¹¶åå…± {len(df_result)} æ¡è®°å½•")
    
    return df_result

def backup_file(file_path, index_name):
    """å¤‡ä»½å•ä¸ªæ–‡ä»¶"""
    if not file_path.exists():
        return
    
    try:
        index_code = file_path.stem
        backup_subdir = BACKUP_DIR / index_code
        backup_subdir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = backup_subdir / f"{index_code}_{timestamp}.parquet"
        
        shutil.copy2(file_path, backup_file)
        logger.debug(f"å·²å¤‡ä»½: {backup_file}")
        
        # æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘3ä¸ªï¼‰
        backups = sorted(backup_subdir.glob(f"{index_code}_*.parquet"), reverse=True)
        for old_backup in backups[3:]:
            old_backup.unlink()
            logger.debug(f"åˆ é™¤æ—§å¤‡ä»½: {old_backup}")
    
    except Exception as e:
        logger.warning(f"{index_name} å¤‡ä»½å¤±è´¥: {e}")

# ============================================================
# ä¸»ç¨‹åº
# ============================================================

def main():
    print("=" * 80)
    print("  ä¸‹è½½å¤§ç›˜æŒ‡æ•°æ•°æ® v2.1 - å¢é‡ä¸‹è½½ç‰ˆï¼ˆä¿®å¤ç‰ˆï¼‰")
    print("  âœ… ä¿®å¤æ—¥æœŸç±»å‹é”™è¯¯")
    print("  âœ… æ™ºèƒ½äº¤æ˜“æ—¥æ£€æµ‹")
    print("  âœ… é¿å…é‡å¤ä¸‹è½½")
    print("=" * 80)
    
    # æ£€æŸ¥ä»Šå¤©æ˜¯å¦æ˜¯äº¤æ˜“æ—¥
    is_trading, days_since = is_trading_day_today()
    today_str = datetime.now().strftime('%Y-%m-%d (%A)')
    
    if is_trading:
        print(f"\nğŸ“… ä»Šå¤©: {today_str} - äº¤æ˜“æ—¥")
    else:
        print(f"\nğŸ“… ä»Šå¤©: {today_str} - éäº¤æ˜“æ—¥ï¼ˆè·ç¦»ä¸Šä¸ªäº¤æ˜“æ—¥çº¦{days_since}å¤©ï¼‰")
    
    print(f"\né…ç½®:")
    print(f"  å¼ºåˆ¶å…¨é‡ä¸‹è½½: {'æ˜¯' if INCREMENTAL_CONFIG['force_full_download'] else 'å¦'}")
    print(f"  å›æº¯å¤©æ•°: {INCREMENTAL_CONFIG['lookback_days']} å¤©")
    print(f"  æœ€å°æ›´æ–°é—´éš”: {INCREMENTAL_CONFIG['min_gap_days']} å¤©")
    print(f"  æ™ºèƒ½äº¤æ˜“æ—¥æ£€æµ‹: {'æ˜¯' if INCREMENTAL_CONFIG['smart_trading_day_check'] else 'å¦'}")
    print(f"  ç»“æŸæ—¥æœŸ: {END_DATE}")
    
    print(f"\næŒ‡æ•°åˆ—è¡¨:")
    for key, index_info in INDICES.items():
        print(f"  - {index_info['name']} (ä¿å­˜ä»£ç : {index_info['code']})")
    
    # ç™»å½• Baostock
    print("\nç™»å½• Baostock...")
    lg = bs.login()
    if lg.error_code != '0':
        logger.error(f"âŒ ç™»å½•å¤±è´¥: {lg.error_msg}")
        print(f"âŒ ç™»å½•å¤±è´¥: {lg.error_msg}")
        return
    
    logger.info("âœ… ç™»å½•æˆåŠŸ")
    print("âœ… ç™»å½•æˆåŠŸ")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total': len(INDICES),
        'updated': 0,
        'skipped': 0,
        'failed': 0,
        'new_records': 0
    }
    
    # ä¸‹è½½æŒ‡æ•°æ•°æ®
    print(f"\nå¼€å§‹æ™ºèƒ½å¢é‡æ›´æ–°...\n")
    
    for key, index_info in tqdm(INDICES.items(), desc="æ›´æ–°è¿›åº¦"):
        index_name = index_info['name']
        save_code = index_info['code']
        download_code = index_info['download_code']
        
        # è¯»å–ç°æœ‰æ•°æ®
        df_existing, latest_date = get_existing_data(save_code)
        
        # è®¡ç®—ä¸‹è½½èŒƒå›´
        start_date, end_date, need_download = calculate_download_range(latest_date, index_name)
        
        if not need_download:
            stats['skipped'] += 1
            continue
        
        # å¤‡ä»½
        if INCREMENTAL_CONFIG['backup_before_update'] and df_existing is not None:
            file_path = OUTPUT_DIR / f"{save_code}.parquet"
            backup_file(file_path, index_name)
        
        # ä¸‹è½½æ–°æ•°æ®
        df_new = download_index_data(download_code, index_name, save_code, start_date, end_date)
        
        if df_new is None:
            stats['failed'] += 1
            continue
        
        # åˆå¹¶æ•°æ®
        df_final = merge_data(df_existing, df_new, index_name)
        
        if df_final is None or df_final.empty:
            stats['failed'] += 1
            continue
        
        # ä¿å­˜Parquetæ ¼å¼
        output_file = OUTPUT_DIR / f"{save_code}.parquet"
        df_final.to_parquet(output_file, index=False)
        
        # åŒæ—¶ä¿å­˜CSVç‰ˆæœ¬
        csv_file = OUTPUT_DIR / f"{save_code}.csv"
        df_final.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        new_records = len(df_new) if not df_new.empty else 0
        stats['new_records'] += new_records
        stats['updated'] += 1
    
    # é€€å‡ºç™»å½•
    bs.logout()
    logger.info("å·²é€€å‡º Baostock")
    
    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 80)
    print("æ›´æ–°å®Œæˆç»Ÿè®¡")
    print("=" * 80)
    print(f"æ€»æŒ‡æ•°æ•°: {stats['total']}")
    print(f"âœ… å·²æ›´æ–°: {stats['updated']}")
    print(f"â­ï¸  å·²è·³è¿‡: {stats['skipped']} (æ•°æ®å·²æ˜¯æœ€æ–°)")
    print(f"âŒ æ›´æ–°å¤±è´¥: {stats['failed']}")
    print(f"ğŸ“Š æ–°å¢è®°å½•: {stats['new_records']} æ¡")
    print("=" * 80)
    
    if stats['updated'] > 0 or stats['skipped'] > 0:
        print(f"\nâœ… æŒ‡æ•°æ•°æ®å·²æ˜¯æœ€æ–°çŠ¶æ€")
        print(f"ğŸ’¡ æ•°æ®ç›®å½•: {OUTPUT_DIR}")
        print(f"ğŸ’¡ å¤‡ä»½ç›®å½•: {BACKUP_DIR}")
    else:
        logger.error("âŒ æ²¡æœ‰æˆåŠŸæ›´æ–°ä»»ä½•æŒ‡æ•°æ•°æ®")

if __name__ == "__main__":
    main()
