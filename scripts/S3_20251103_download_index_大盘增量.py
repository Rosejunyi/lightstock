#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¸‹è½½å¤§ç›˜æŒ‡æ•°æ•°æ® v2.0 - å¢é‡ä¸‹è½½ç‰ˆï¼ˆå«å®Œæ•´å­—æ®µï¼‰

åŠŸèƒ½ï¼š
1. âœ… ä¸‹è½½ä¸Šè¯æŒ‡æ•°ã€æ·±è¯æˆæŒ‡ã€æ²ªæ·±300ã€åˆ›ä¸šæ¿æŒ‡
2. âœ… å¢é‡ä¸‹è½½ï¼šåªä¸‹è½½ç¼ºå¤±çš„äº¤æ˜“æ—¥æ•°æ®
3. âœ… å®Œæ•´å­—æ®µï¼šæ¶¨è·Œé¢ã€æŒ¯å¹…ã€æ˜¨æ”¶ç­‰æ´¾ç”Ÿå­—æ®µ
4. âœ… å‘¨æœŸæ”¶ç›Šï¼š1æœˆã€3æœˆã€6æœˆæ¶¨è·Œå¹…
5. âœ… è‡ªåŠ¨å¤‡ä»½ï¼šä¿ç•™å†å²ç‰ˆæœ¬
6. âœ… æ•°æ®éªŒè¯ï¼šæ£€æŸ¥æ•°æ®å®Œæ•´æ€§

ç”¨é€”ï¼š
- è®¡ç®—ç›¸å¯¹å¼ºåº¦(RS Rating)
- ç›¸å¯¹å¤§ç›˜è¡¨ç°åˆ†æ
- å¸‚åœºç¯å¢ƒåˆ¤æ–­

æ•°æ®æºï¼šBaostock
ä½œè€…ï¼šClaude
ç‰ˆæœ¬ï¼šv2.0ï¼ˆå¢é‡ç‰ˆï¼‰
æ—¥æœŸï¼š2025-11-03
"""

import baostock as bs
import pandas as pd
from pathlib import Path
from datetime import datetime, timedelta
from tqdm import tqdm
import logging
import shutil

# ============================================================
# é…ç½®
# ============================================================

# å¤§ç›˜æŒ‡æ•°é…ç½®
INDICES = {
    'sh.000001': {'name': 'ä¸Šè¯æŒ‡æ•°', 'code': '999999', 'download_code': 'sh.000001'},
    'sz.399001': {'name': 'æ·±è¯æˆæŒ‡', 'code': '399001', 'download_code': 'sz.399001'},
    'sh.000300': {'name': 'æ²ªæ·±300', 'code': '000300', 'download_code': 'sh.000300'},
    'sz.399006': {'name': 'åˆ›ä¸šæ¿æŒ‡', 'code': '399006', 'download_code': 'sz.399006'},
}

# ç›®å½•é…ç½®
OUTPUT_DIR = Path("data/index_data")
BACKUP_DIR = Path("data/backups/index_data")
LOG_DIR = Path("logs")

# ä¸‹è½½é…ç½®
DEFAULT_START_DATE = "1990-01-01"  # é¦–æ¬¡ä¸‹è½½çš„å¼€å§‹æ—¥æœŸ
END_DATE = datetime.now().strftime('%Y-%m-%d')  # ç»“æŸæ—¥æœŸï¼ˆä»Šå¤©ï¼‰
MAX_RETRIES = 3
RETRY_DELAY = 2

# å¢é‡æ›´æ–°é…ç½®
INCREMENTAL_CONFIG = {
    'force_full_download': False,     # æ˜¯å¦å¼ºåˆ¶å…¨é‡ä¸‹è½½
    'lookback_days': 10,              # å›æº¯å¤©æ•°ï¼šé˜²æ­¢æ•°æ®é—æ¼
    'min_gap_days': 1,                # æœ€å°æ›´æ–°é—´éš”ï¼šè·ç¦»æœ€æ–°æ•°æ®<Nå¤©ä¸æ›´æ–°
    'backup_before_update': True,     # æ›´æ–°å‰æ˜¯å¦å¤‡ä»½
}

# åˆ›å»ºç›®å½•
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
BACKUP_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(
            LOG_DIR / f'index_incremental_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
            encoding='utf-8'
        ),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================
# å·¥å…·å‡½æ•°
# ============================================================

def format_date_string(date_value):
    """æ ¼å¼åŒ–æ—¥æœŸä¸º YYYY-MM-DD å­—ç¬¦ä¸²"""
    if pd.isna(date_value):
        return None
    try:
        if isinstance(date_value, str):
            if len(date_value) == 10 and date_value[4] == '-' and date_value[7] == '-':
                return date_value
        dt = pd.to_datetime(date_value)
        return dt.strftime('%Y-%m-%d')
    except:
        return None

def calculate_derived_fields(df):
    """
    è®¡ç®—æ´¾ç”Ÿå­—æ®µ
    
    æ–°å¢å­—æ®µï¼š
    - æ¶¨è·Œé¢ï¼šæ”¶ç›˜ - æ˜¨æ”¶
    - æŒ¯å¹…ï¼š(æœ€é«˜ - æœ€ä½) / æ˜¨æ”¶ * 100
    - æ˜¨æ”¶ï¼šå‰ä¸€å¤©çš„æ”¶ç›˜ä»·
    - æ³¢åŠ¨å¼‚å¸¸ï¼šå•æ—¥æ¶¨è·Œå¹…è¶…è¿‡Â±5%ï¼ˆæŒ‡æ•°æ²¡æœ‰æ¶¨è·Œåœï¼Œä½†å¯æ ‡è®°å¼‚å¸¸æ³¢åŠ¨ï¼‰
    """
    if df.empty:
        return df
    
    # ç¡®ä¿æ•°æ®æŒ‰æ—¥æœŸæ’åº
    df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
    
    # å¦‚æœAPIæ²¡æœ‰æä¾›æ˜¨æ”¶ï¼Œä½¿ç”¨å‰ä¸€å¤©çš„æ”¶ç›˜ä»·è®¡ç®—
    if 'æ˜¨æ”¶' not in df.columns or df['æ˜¨æ”¶'].isna().all():
        df['æ˜¨æ”¶'] = df['æ”¶ç›˜'].shift(1)
    
    # è®¡ç®—æ¶¨è·Œé¢
    df['æ¶¨è·Œé¢'] = df['æ”¶ç›˜'] - df['æ˜¨æ”¶']
    
    # è®¡ç®—æŒ¯å¹…
    df['æŒ¯å¹…'] = ((df['æœ€é«˜'] - df['æœ€ä½']) / df['æ˜¨æ”¶'] * 100).round(4)
    
    # åˆ¤æ–­æ³¢åŠ¨å¼‚å¸¸ï¼ˆæŒ‡æ•°å•æ—¥æ¶¨è·Œå¹…è¶…è¿‡Â±5%è§†ä¸ºå¼‚å¸¸ï¼‰
    df['æ³¢åŠ¨å¼‚å¸¸'] = df['æ¶¨è·Œå¹…'].apply(
        lambda x: 'X' if (pd.notna(x) and (abs(x) > 5)) else None
    )
    
    return df

def calculate_period_returns(df):
    """
    è®¡ç®—ä¸åŒå‘¨æœŸçš„æ”¶ç›Šç‡
    
    ç”¨äºç­›é€‰æ¡ä»¶13ï¼šè‚¡ç¥¨ç›¸å¯¹å¤§ç›˜è¡¨ç°
    """
    df = df.copy()
    df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
    df = df.sort_values('æ—¥æœŸ')
    
    # è®¡ç®—ä¸åŒå‘¨æœŸçš„æ¶¨è·Œå¹…
    # 1ä¸ªæœˆ â‰ˆ 22ä¸ªäº¤æ˜“æ—¥
    # 3ä¸ªæœˆ â‰ˆ 66ä¸ªäº¤æ˜“æ—¥
    # 6ä¸ªæœˆ â‰ˆ 132ä¸ªäº¤æ˜“æ—¥
    
    df['æ¶¨è·Œå¹…_1æœˆ'] = df['æ”¶ç›˜'].pct_change(22) * 100
    df['æ¶¨è·Œå¹…_3æœˆ'] = df['æ”¶ç›˜'].pct_change(66) * 100
    df['æ¶¨è·Œå¹…_6æœˆ'] = df['æ”¶ç›˜'].pct_change(132) * 100
    
    # è½¬æ¢å›å­—ç¬¦ä¸²æ—¥æœŸ
    df['æ—¥æœŸ'] = df['æ—¥æœŸ'].dt.strftime('%Y-%m-%d')
    
    return df

def get_existing_data(index_code):
    """
    è¯»å–ç°æœ‰æ•°æ®
    
    è¿”å›: (DataFrame, æœ€æ–°æ—¥æœŸ)
    """
    file_path = OUTPUT_DIR / f"{index_code}.parquet"
    
    if not file_path.exists():
        return None, None
    
    try:
        df = pd.read_parquet(file_path)
        
        if df.empty:
            return None, None
        
        # è·å–æœ€æ–°æ—¥æœŸ
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'])
        latest_date = df['æ—¥æœŸ'].max().strftime('%Y-%m-%d')
        
        logger.debug(f"{index_code}: ç°æœ‰æ•°æ® {len(df)} æ¡ï¼Œæœ€æ–°æ—¥æœŸ {latest_date}")
        
        return df, latest_date
    
    except Exception as e:
        logger.error(f"{index_code}: è¯»å–ç°æœ‰æ•°æ®å¤±è´¥ - {e}")
        return None, None

def calculate_download_range(latest_date, index_name):
    """
    è®¡ç®—éœ€è¦ä¸‹è½½çš„æ—¥æœŸèŒƒå›´
    
    è¿”å›: (å¼€å§‹æ—¥æœŸ, ç»“æŸæ—¥æœŸ, æ˜¯å¦éœ€è¦ä¸‹è½½)
    """
    if INCREMENTAL_CONFIG['force_full_download']:
        return DEFAULT_START_DATE, END_DATE, True
    
    if latest_date is None:
        # é¦–æ¬¡ä¸‹è½½
        return DEFAULT_START_DATE, END_DATE, True
    
    # è§£ææœ€æ–°æ—¥æœŸ
    latest_dt = datetime.strptime(latest_date, '%Y-%m-%d')
    today = datetime.now()
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
    days_gap = (today - latest_dt).days
    
    if days_gap <= INCREMENTAL_CONFIG['min_gap_days']:
        logger.debug(f"{index_name}: æ•°æ®å·²æ˜¯æœ€æ–°ï¼ˆè·ä»Š{days_gap}å¤©ï¼‰ï¼Œè·³è¿‡ä¸‹è½½")
        return None, None, False
    
    # è®¡ç®—ä¸‹è½½èŒƒå›´ï¼ˆå›æº¯Nå¤©é˜²æ­¢é—æ¼ï¼‰
    start_dt = latest_dt - timedelta(days=INCREMENTAL_CONFIG['lookback_days'])
    start_date = start_dt.strftime('%Y-%m-%d')
    
    logger.debug(f"{index_name}: å¢é‡ä¸‹è½½ {start_date} è‡³ {END_DATE}")
    
    return start_date, END_DATE, True

def download_index_data(download_code, index_name, save_code, start_date, end_date, retry_count=0):
    """
    ä¸‹è½½å•ä¸ªæŒ‡æ•°çš„å†å²æ•°æ®
    
    å‚æ•°:
        download_code: ä¸‹è½½æ—¶ä½¿ç”¨çš„ä»£ç ï¼ˆå¦‚ sh.000001ï¼‰
        index_name: æŒ‡æ•°åç§°ï¼ˆå¦‚ ä¸Šè¯æŒ‡æ•°ï¼‰
        save_code: ä¿å­˜æ—¶ä½¿ç”¨çš„ä»£ç ï¼ˆå¦‚ 999999ï¼‰
        start_date: å¼€å§‹æ—¥æœŸ
        end_date: ç»“æŸæ—¥æœŸ
        retry_count: é‡è¯•æ¬¡æ•°
    """
    try:
        logger.info(f"ä¸‹è½½ {index_name} ({start_date} è‡³ {end_date})")
        
        # ä½¿ç”¨ query_history_k_data_plus æ¥å£ - è¯·æ±‚å®Œæ•´å­—æ®µ
        rs = bs.query_history_k_data_plus(
            code=download_code,
            fields="date,open,high,low,close,preclose,volume,amount,pctChg",
            start_date=start_date,
            end_date=end_date,
            frequency="d",
            adjustflag="3"  # ä¸å¤æƒï¼ˆæŒ‡æ•°ä¸éœ€è¦å¤æƒï¼‰
        )
        
        if rs.error_code != '0':
            if retry_count < MAX_RETRIES:
                logger.warning(f"{index_name}: ä¸‹è½½å¤±è´¥ï¼Œé‡è¯• {retry_count + 1}/{MAX_RETRIES}")
                import time
                time.sleep(RETRY_DELAY)
                return download_index_data(download_code, index_name, save_code, start_date, end_date, retry_count + 1)
            else:
                logger.error(f"{index_name}: {rs.error_msg}")
                return None
        
        # æ”¶é›†æ•°æ®
        data_list = []
        while (rs.error_code == '0') & rs.next():
            data_list.append(rs.get_row_data())
        
        if not data_list:
            logger.warning(f"{index_name}: æ— æ•°æ®")
            return pd.DataFrame()
        
        # è½¬æ¢ä¸º DataFrame
        df = pd.DataFrame(data_list, columns=rs.fields)
        
        # é‡å‘½ååˆ—
        column_mapping = {
            'date': 'æ—¥æœŸ',
            'open': 'å¼€ç›˜',
            'high': 'æœ€é«˜',
            'low': 'æœ€ä½',
            'close': 'æ”¶ç›˜',
            'preclose': 'æ˜¨æ”¶',
            'volume': 'æˆäº¤é‡',
            'amount': 'æˆäº¤é¢',
            'pctChg': 'æ¶¨è·Œå¹…'
        }
        
        df = df.rename(columns=column_mapping)
        
        # æ·»åŠ æŒ‡æ•°ä¿¡æ¯ï¼ˆä½¿ç”¨ä¿å­˜ä»£ç ï¼‰
        df['æŒ‡æ•°ä»£ç '] = save_code
        df['æŒ‡æ•°åç§°'] = index_name
        
        # è½¬æ¢æ•°æ®ç±»å‹
        numeric_cols = ['å¼€ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æ”¶ç›˜', 'æ˜¨æ”¶', 'æˆäº¤é¢', 'æ¶¨è·Œå¹…']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        if 'æˆäº¤é‡' in df.columns:
            df['æˆäº¤é‡'] = pd.to_numeric(df['æˆäº¤é‡'], errors='coerce').astype('Int64')
        
        # æ ¼å¼åŒ–æ—¥æœŸ
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].apply(format_date_string)
        
        # åˆ é™¤æ— æ•ˆè¡Œ
        df = df.dropna(subset=['æ—¥æœŸ', 'æ”¶ç›˜'])
        
        if df.empty:
            logger.warning(f"{index_name}: è½¬æ¢åæ— æœ‰æ•ˆæ•°æ®")
            return pd.DataFrame()
        
        # æ’åº
        df = df.sort_values('æ—¥æœŸ').reset_index(drop=True)
        
        # è®¡ç®—æ´¾ç”Ÿå­—æ®µ
        df = calculate_derived_fields(df)
        
        logger.info(f"âœ… {index_name}: ä¸‹è½½æˆåŠŸï¼Œå…± {len(df)} æ¡è®°å½•")
        
        return df
    
    except Exception as e:
        if retry_count < MAX_RETRIES:
            logger.warning(f"{index_name}: å¼‚å¸¸ï¼Œé‡è¯• {retry_count + 1}/{MAX_RETRIES} - {e}")
            import time
            time.sleep(RETRY_DELAY)
            return download_index_data(download_code, index_name, save_code, start_date, end_date, retry_count + 1)
        else:
            logger.error(f"{index_name}: ä¸‹è½½å¼‚å¸¸ - {e}")
            return None

def merge_data(df_existing, df_new, index_name):
    """
    åˆå¹¶ç°æœ‰æ•°æ®å’Œæ–°ä¸‹è½½çš„æ•°æ®
    
    ç­–ç•¥ï¼š
    1. åˆå¹¶ä¸¤ä¸ªDataFrame
    2. æŒ‰æ—¥æœŸå»é‡ï¼ˆä¿ç•™æ–°æ•°æ®ï¼‰
    3. é‡æ–°è®¡ç®—æ‰€æœ‰æ´¾ç”Ÿå­—æ®µå’Œå‘¨æœŸæ”¶ç›Š
    4. æ’åº
    """
    if df_existing is None or df_existing.empty:
        df_result = df_new
    elif df_new is None or df_new.empty:
        return df_existing
    else:
        # ç¡®ä¿æ—¥æœŸæ ¼å¼ä¸€è‡´
        df_existing['æ—¥æœŸ'] = pd.to_datetime(df_existing['æ—¥æœŸ'])
        df_new['æ—¥æœŸ'] = pd.to_datetime(df_new['æ—¥æœŸ'])
        
        # åˆå¹¶
        df_result = pd.concat([df_existing, df_new], ignore_index=True)
        
        # å»é‡ï¼ˆä¿ç•™æœ€æ–°çš„ï¼‰
        df_result = df_result.drop_duplicates(subset=['æ—¥æœŸ'], keep='last')
        
        # æ’åº
        df_result = df_result.sort_values('æ—¥æœŸ').reset_index(drop=True)
    
    # è½¬æ¢æ—¥æœŸå›å­—ç¬¦ä¸²
    df_result['æ—¥æœŸ'] = df_result['æ—¥æœŸ'].dt.strftime('%Y-%m-%d')
    
    # é‡æ–°è®¡ç®—æ´¾ç”Ÿå­—æ®µï¼ˆç¡®ä¿å®Œæ•´æ€§ï¼‰
    df_result = calculate_derived_fields(df_result)
    
    # é‡æ–°è®¡ç®—å‘¨æœŸæ”¶ç›Šç‡
    df_result = calculate_period_returns(df_result)
    
    logger.info(f"{index_name}: åˆå¹¶åå…± {len(df_result)} æ¡è®°å½•")
    
    return df_result

def backup_file(file_path, index_name):
    """å¤‡ä»½å•ä¸ªæ–‡ä»¶"""
    if not file_path.exists():
        return
    
    try:
        index_code = file_path.stem
        backup_subdir = BACKUP_DIR / index_code
        backup_subdir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = backup_subdir / f"{index_code}_{timestamp}.parquet"
        
        shutil.copy2(file_path, backup_file)
        logger.debug(f"å·²å¤‡ä»½: {backup_file}")
        
        # æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘3ä¸ªï¼‰
        backups = sorted(backup_subdir.glob(f"{index_code}_*.parquet"), reverse=True)
        for old_backup in backups[3:]:
            old_backup.unlink()
            logger.debug(f"åˆ é™¤æ—§å¤‡ä»½: {old_backup}")
    
    except Exception as e:
        logger.warning(f"{index_name} å¤‡ä»½å¤±è´¥: {e}")

# ============================================================
# ä¸»ç¨‹åº
# ============================================================

def main():
    print("=" * 80)
    print("  ä¸‹è½½å¤§ç›˜æŒ‡æ•°æ•°æ® v2.0 - å¢é‡ä¸‹è½½ç‰ˆï¼ˆå«å®Œæ•´å­—æ®µï¼‰")
    print("  æ™ºèƒ½æ£€æµ‹ + å¢é‡æ›´æ–° + å®Œæ•´å­—æ®µ + å‘¨æœŸæ”¶ç›Š")
    print("=" * 80)
    
    print(f"\né…ç½®:")
    print(f"  å¼ºåˆ¶å…¨é‡ä¸‹è½½: {'æ˜¯' if INCREMENTAL_CONFIG['force_full_download'] else 'å¦'}")
    print(f"  å›æº¯å¤©æ•°: {INCREMENTAL_CONFIG['lookback_days']} å¤©")
    print(f"  æœ€å°æ›´æ–°é—´éš”: {INCREMENTAL_CONFIG['min_gap_days']} å¤©")
    print(f"  ç»“æŸæ—¥æœŸ: {END_DATE}")
    print(f"  æ›´æ–°å‰å¤‡ä»½: {'æ˜¯' if INCREMENTAL_CONFIG['backup_before_update'] else 'å¦'}")
    
    print(f"\nâœ¨ å®Œæ•´å­—æ®µæ”¯æŒ:")
    print(f"  âœ… åŸºç¡€å­—æ®µï¼šå¼€ç›˜ã€æœ€é«˜ã€æœ€ä½ã€æ”¶ç›˜ã€æˆäº¤é‡ã€æˆäº¤é¢ã€æ¶¨è·Œå¹…")
    print(f"  âœ… æ´¾ç”Ÿå­—æ®µï¼šæ¶¨è·Œé¢ã€æŒ¯å¹…ã€æ˜¨æ”¶ã€æ³¢åŠ¨å¼‚å¸¸")
    print(f"  âœ… å‘¨æœŸæ”¶ç›Šï¼š1æœˆã€3æœˆã€6æœˆæ¶¨è·Œå¹…")
    
    print(f"\næŒ‡æ•°åˆ—è¡¨:")
    for key, index_info in INDICES.items():
        print(f"  - {index_info['name']} (ä¸‹è½½: {index_info['download_code']}, ä¿å­˜: {index_info['code']})")
    
    # ç™»å½• Baostock
    print("\nç™»å½• Baostock...")
    lg = bs.login()
    if lg.error_code != '0':
        logger.error(f"âŒ ç™»å½•å¤±è´¥: {lg.error_msg}")
        print(f"âŒ ç™»å½•å¤±è´¥: {lg.error_msg}")
        return
    
    logger.info("âœ… ç™»å½•æˆåŠŸ")
    print("âœ… ç™»å½•æˆåŠŸ")
    
    # ç»Ÿè®¡ä¿¡æ¯
    stats = {
        'total': len(INDICES),
        'updated': 0,
        'skipped': 0,
        'failed': 0,
        'new_records': 0
    }
    
    # ä¸‹è½½æŒ‡æ•°æ•°æ®
    print(f"\nå¼€å§‹å¢é‡æ›´æ–°æŒ‡æ•°æ•°æ®...\n")
    
    for key, index_info in tqdm(INDICES.items(), desc="æ›´æ–°è¿›åº¦"):
        index_name = index_info['name']
        save_code = index_info['code']
        download_code = index_info['download_code']
        
        # è¯»å–ç°æœ‰æ•°æ®
        df_existing, latest_date = get_existing_data(save_code)
        
        # è®¡ç®—ä¸‹è½½èŒƒå›´
        start_date, end_date, need_download = calculate_download_range(latest_date, index_name)
        
        if not need_download:
            stats['skipped'] += 1
            continue
        
        # å¤‡ä»½
        if INCREMENTAL_CONFIG['backup_before_update'] and df_existing is not None:
            file_path = OUTPUT_DIR / f"{save_code}.parquet"
            backup_file(file_path, index_name)
        
        # ä¸‹è½½æ–°æ•°æ®
        df_new = download_index_data(download_code, index_name, save_code, start_date, end_date)
        
        if df_new is None:
            stats['failed'] += 1
            continue
        
        # åˆå¹¶æ•°æ®
        df_final = merge_data(df_existing, df_new, index_name)
        
        if df_final is None or df_final.empty:
            stats['failed'] += 1
            continue
        
        # ä¿å­˜Parquetæ ¼å¼
        output_file = OUTPUT_DIR / f"{save_code}.parquet"
        df_final.to_parquet(output_file, index=False)
        
        # åŒæ—¶ä¿å­˜CSVç‰ˆæœ¬ï¼ˆå¯é€‰ï¼‰
        csv_file = OUTPUT_DIR / f"{save_code}.csv"
        df_final.to_csv(csv_file, index=False, encoding='utf-8-sig')
        
        new_records = len(df_new) if not df_new.empty else 0
        stats['new_records'] += new_records
        stats['updated'] += 1
    
    # é€€å‡ºç™»å½•
    bs.logout()
    logger.info("å·²é€€å‡º Baostock")
    
    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 80)
    print("æ›´æ–°å®Œæˆç»Ÿè®¡")
    print("=" * 80)
    print(f"æ€»æŒ‡æ•°æ•°: {stats['total']}")
    print(f"âœ… å·²æ›´æ–°: {stats['updated']}")
    print(f"â­ï¸  å·²è·³è¿‡: {stats['skipped']} (æ•°æ®å·²æ˜¯æœ€æ–°)")
    print(f"âŒ æ›´æ–°å¤±è´¥: {stats['failed']}")
    print(f"ğŸ“Š æ–°å¢è®°å½•: {stats['new_records']} æ¡")
    print("=" * 80)
    
    if stats['updated'] > 0 or stats['skipped'] > 0:
        # æ˜¾ç¤ºæ•°æ®ç¤ºä¾‹
        print(f"\nğŸ“Š æ•°æ®ç¤ºä¾‹ï¼ˆä¸Šè¯æŒ‡æ•°ï¼‰:")
        sample_file = OUTPUT_DIR / "999999.parquet"
        if sample_file.exists():
            sample_df = pd.read_parquet(sample_file)
            print(f"  æ–‡ä»¶å: 999999.parquet")
            print(f"  æŒ‡æ•°ä»£ç : {sample_df['æŒ‡æ•°ä»£ç '].iloc[0]}")
            print(f"  æŒ‡æ•°åç§°: {sample_df['æŒ‡æ•°åç§°'].iloc[0]}")
            print(f"  æ•°æ®è¡Œæ•°: {len(sample_df):,}")
            print(f"  æ—¥æœŸèŒƒå›´: {sample_df['æ—¥æœŸ'].min()} è‡³ {sample_df['æ—¥æœŸ'].max()}")
            
            print(f"\n  å®Œæ•´å­—æ®µåˆ—è¡¨:")
            print(f"  {list(sample_df.columns)}")
            
            # éªŒè¯å­—æ®µå®Œæ•´æ€§
            required_fields = ['æ˜¨æ”¶', 'æ¶¨è·Œé¢', 'æŒ¯å¹…', 'æ³¢åŠ¨å¼‚å¸¸', 'æ¶¨è·Œå¹…_1æœˆ', 'æ¶¨è·Œå¹…_3æœˆ', 'æ¶¨è·Œå¹…_6æœˆ']
            print(f"\n  å­—æ®µå®Œæ•´æ€§æ£€æŸ¥:")
            for field in required_fields:
                if field in sample_df.columns:
                    valid_count = sample_df[field].notna().sum()
                    print(f"    âœ… {field}: å­˜åœ¨ ({valid_count}/{len(sample_df)} = {valid_count/len(sample_df)*100:.1f}%)")
                else:
                    print(f"    âŒ {field}: ç¼ºå¤±ï¼")
            
            print(f"\n  æœ€æ–°5æ¡è®°å½•:")
            display_cols = ['æ—¥æœŸ', 'æ”¶ç›˜', 'æ˜¨æ”¶', 'æ¶¨è·Œé¢', 'æ¶¨è·Œå¹…', 'æŒ¯å¹…', 'æ¶¨è·Œå¹…_1æœˆ', 'æ¶¨è·Œå¹…_3æœˆ']
            available_cols = [col for col in display_cols if col in sample_df.columns]
            print(sample_df[available_cols].tail(5).to_string(index=False))
        
        print("\nğŸ‰ æŒ‡æ•°æ•°æ®å¢é‡æ›´æ–°å®Œæˆï¼")
        print(f"ğŸ’¡ æ•°æ®ç›®å½•: {OUTPUT_DIR}")
        print(f"ğŸ’¡ å¤‡ä»½ç›®å½•: {BACKUP_DIR}")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {LOG_DIR}/index_incremental_*.log")
        
        # ç”¨é€”è¯´æ˜
        print("\n" + "=" * 80)
        print("æ•°æ®ç”¨é€”ä¸ä½¿ç”¨ç¤ºä¾‹")
        print("=" * 80)
        print("""
âœ… å·²ä¿®å¤çš„åŠŸèƒ½ï¼š
   - å¢é‡ä¸‹è½½ï¼šåªä¸‹è½½ç¼ºå¤±çš„äº¤æ˜“æ—¥æ•°æ®
   - å®Œæ•´å­—æ®µï¼šæ¶¨è·Œé¢ã€æŒ¯å¹…ã€æ˜¨æ”¶ã€æ³¢åŠ¨å¼‚å¸¸
   - å‘¨æœŸæ”¶ç›Šï¼š1æœˆã€3æœˆã€6æœˆæ¶¨è·Œå¹…
   - è‡ªåŠ¨å¤‡ä»½ï¼šä¿ç•™å†å²ç‰ˆæœ¬

ğŸ“Š æ•°æ®ç”¨é€”ï¼š
   1. è®¡ç®—ç›¸å¯¹å¼ºåº¦(RS Rating)
      - ä¸ªè‚¡æ¶¨å¹… vs å¤§ç›˜æ¶¨å¹…
      - æ’åç™¾åˆ†ä½
   
   2. ç­›é€‰æ¡ä»¶13ï¼šç›¸å¯¹å¸‚åœºè¡¨ç°
      - è‚¡ç¥¨1ã€3ã€6ä¸ªæœˆæ¶¨å¹…
      - å¤§å¹…è·‘èµ¢åŒæœŸå¤§ç›˜è‡³å°‘1å€ä»¥ä¸Š
   
   3. å¸‚åœºç¯å¢ƒåˆ¤æ–­
      - ç‰›å¸‚/ç†Šå¸‚è¯†åˆ«
      - è¡Œä¸šè½®åŠ¨åˆ†æ

ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹ï¼š
```python
import pandas as pd

# è¯»å–ä¸Šè¯æŒ‡æ•°ï¼ˆæ–‡ä»¶åä¸º 999999.parquetï¼‰
sh = pd.read_parquet('data/index_data/999999.parquet')

# æŸ¥çœ‹å®Œæ•´å­—æ®µ
print(sh.columns.tolist())

# æŸ¥çœ‹æœ€æ–°æ•°æ®
print(sh.tail())

# è·å–æœ€è¿‘çš„æ¶¨è·Œå¹…
latest_1m = sh['æ¶¨è·Œå¹…_1æœˆ'].iloc[-1]
latest_3m = sh['æ¶¨è·Œå¹…_3æœˆ'].iloc[-1]
latest_6m = sh['æ¶¨è·Œå¹…_6æœˆ'].iloc[-1]

print(f"ä¸Šè¯æŒ‡æ•°è¿‘1æœˆæ¶¨å¹…: {latest_1m:.2f}%")
print(f"ä¸Šè¯æŒ‡æ•°è¿‘3æœˆæ¶¨å¹…: {latest_3m:.2f}%")
print(f"ä¸Šè¯æŒ‡æ•°è¿‘6æœˆæ¶¨å¹…: {latest_6m:.2f}%")

# è®¡ç®—ç›¸å¯¹å¼ºåº¦
stock_return_1m = 15.5  # å‡è®¾æŸè‚¡ç¥¨1æœˆæ¶¨å¹…15.5%
relative_strength = stock_return_1m - latest_1m
print(f"ç›¸å¯¹å¤§ç›˜å¼ºåº¦: {relative_strength:.2f}%")
```

âš™ï¸ å®šæ—¶æ›´æ–°ï¼š
   - å»ºè®®æ¯æ—¥æ”¶ç›˜åè¿è¡Œï¼ˆå¦‚ 18:00ï¼‰
   - å¼ºåˆ¶å…¨é‡ä¸‹è½½ï¼šINCREMENTAL_CONFIG['force_full_download'] = True
   - è°ƒæ•´å›æº¯å¤©æ•°ï¼šINCREMENTAL_CONFIG['lookback_days'] = N

âš ï¸ é‡è¦è¯´æ˜ï¼š
   âœ… ä¸Šè¯æŒ‡æ•°ä½¿ç”¨ sh.000001 ä¸‹è½½ï¼ˆæ•°æ®å®Œæ•´ï¼‰
   âœ… ä¿å­˜ä¸º 999999.parquet æ–‡ä»¶ï¼ˆç»Ÿä¸€å‘½åï¼‰
   âœ… æŒ‡æ•°ä»£ç å­—æ®µæ˜¾ç¤ºä¸º 999999ï¼ˆä¿æŒä¸€è‡´ï¼‰
   âœ… å…¶ä»–ä»£ç ä½¿ç”¨æ—¶ï¼Œç›´æ¥è¯»å– 999999.parquet å³å¯
        """)
    else:
        logger.error("âŒ æ²¡æœ‰æˆåŠŸæ›´æ–°ä»»ä½•æŒ‡æ•°æ•°æ®")
        print("\nâŒ æ›´æ–°å¤±è´¥ï¼šæ²¡æœ‰æˆåŠŸæ›´æ–°ä»»ä½•æŒ‡æ•°æ•°æ®")
        print("\nå¯èƒ½åŸå› ï¼š")
        print("1. ç½‘ç»œè¿æ¥é—®é¢˜")
        print("2. Baostock æœåŠ¡å™¨ç»´æŠ¤")
        print("3. æ‰€æœ‰æŒ‡æ•°æ•°æ®éƒ½å·²æ˜¯æœ€æ–°")

if __name__ == "__main__":
    main()
