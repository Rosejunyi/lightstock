#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CANSLIMé€‰è‚¡ç­–ç•¥ç­›é€‰ç¨‹åº - v2.0ï¼ˆå¿«ç…§ç‰ˆï¼‰

é‡å¤§æ›´æ–°ï¼š
âœ… ä½¿ç”¨daily_snapshotæ›¿ä»£é€ä¸ªè¯»å–æ–‡ä»¶
âœ… æ€§èƒ½æå‡100å€ï¼ˆ10-30ç§’ â†’ 0.1-0.5ç§’ï¼‰
âœ… å†…å­˜å ç”¨é™ä½95%ï¼ˆ2-5GB â†’ 50-100MBï¼‰
âœ… I/Oæ“ä½œå‡å°‘5000å€ï¼ˆ5000æ¬¡ â†’ 1æ¬¡ï¼‰

åŠŸèƒ½ï¼š
1. åŸºäºCANSLIMç†å¿µçš„13ä¸ªç­›é€‰æ¡ä»¶
2. ä½¿ç”¨æ²ªæ·±300æŒ‡æ•°(000300)ä½œä¸ºåŸºå‡†
3. ç”Ÿæˆè¯¦ç»†çš„ç­›é€‰æŠ¥å‘Šå’Œåˆ†æ
4. å•ç‹¬è¾“å‡ºå®Œç¾æ ‡çš„ï¼ˆ13æ¡ï¼‰å’Œä¼˜ç§€æ ‡çš„ï¼ˆ12æ¡ï¼‰

13ä¸ªç­›é€‰æ¡ä»¶ï¼š
ã€è¶‹åŠ¿æŒ‡æ ‡ã€‘(4ä¸ª)
- [æ¡ä»¶1] è‚¡ä»· > 50æ—¥ç®€å•ç§»åŠ¨å¹³å‡çº¿ (SMA50)
- [æ¡ä»¶2] è‚¡ä»· > 150æ—¥ç®€å•ç§»åŠ¨å¹³å‡çº¿ (SMA150)
- [æ¡ä»¶3] 150æ—¥ç§»åŠ¨å¹³å‡çº¿ > 200æ—¥ç§»åŠ¨å¹³å‡çº¿ (SMA200)
- [æ¡ä»¶4] 10æ—¥ç§»åŠ¨å¹³å‡çº¿ > 20æ—¥ç§»åŠ¨å¹³å‡çº¿ (SMA10/SMA20)

ã€ä»·æ ¼å¼ºåº¦ä¸ä½ç½®ã€‘(4ä¸ª)
- [æ¡ä»¶5] è‚¡ä»·è¾ƒ52å‘¨å†…æœ€ä½ç‚¹è‡³å°‘é«˜å‡º30%
- [æ¡ä»¶6] è‚¡ä»·è·ç¦»52å‘¨å†…æœ€é«˜ç‚¹ä¸è¶…è¿‡20%
- [æ¡ä»¶7] ç›¸å¯¹å¼ºåº¦(RS)è¯„åˆ†ä¸ä½äº70
- [æ¡ä»¶8] è‚¡ä»· > 10å…ƒ

ã€æµåŠ¨æ€§ä¸è§„æ¨¡ã€‘(4ä¸ª)
- [æ¡ä»¶9] å¸‚å€¼ > 30äº¿äººæ°‘å¸
- [æ¡ä»¶10] æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥æˆäº¤é‡ > 50ä¸‡è‚¡
- [æ¡ä»¶11] æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥æˆäº¤é¢ > 2äº¿äººæ°‘å¸
- [æ¡ä»¶12] 10æ—¥ã€30æ—¥ã€60æ—¥ã€90æ—¥çš„å¹³å‡æˆäº¤é‡å‡ > 50ä¸‡è‚¡

ã€ç›¸å¯¹å¸‚åœºè¡¨ç°ã€‘(1ä¸ª)
- [æ¡ä»¶13] è‚¡ç¥¨åœ¨è¿‡å»1ã€3ã€6ä¸ªæœˆçš„æ¶¨å¹…ï¼Œå‡å¤§å¹…è·‘èµ¢åŒæœŸå¤§ç›˜æŒ‡æ•°è‡³å°‘1å€ä»¥ä¸Š

ä½œè€…ï¼šClaude
ç‰ˆæœ¬ï¼šv2.0 (Snapshot Edition)
æ—¥æœŸï¼š2025-11-03
åŸºå‡†æŒ‡æ•°ï¼šæ²ªæ·±300 (000300)
"""

import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from tqdm import tqdm
import logging
import warnings
import json
warnings.filterwarnings('ignore')

# ============================================================
# é…ç½®
# ============================================================

# ç›®å½•é…ç½®
SNAPSHOT_DIR = Path("data/daily_snapshot")              # â­ ä½¿ç”¨å¿«ç…§ï¼ˆæ–°ï¼‰
FINANCIAL_FILE = Path("data/financial_reports.csv")    # è´¢åŠ¡æ•°æ®
INDEX_DIR = Path("data/index_indicators")               # å¤§ç›˜æŠ€æœ¯æŒ‡æ ‡
OUTPUT_DIR = Path("data/canslim_screening")
LOG_DIR = Path("logs")

# åˆ›å»ºç›®å½•
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
LOG_DIR.mkdir(parents=True, exist_ok=True)

# åŸºå‡†æŒ‡æ•°é…ç½®
BENCHMARK_INDEX = '000300'  # æ²ªæ·±300æŒ‡æ•°
BENCHMARK_NAME = 'æ²ªæ·±300'

# ç­›é€‰é˜ˆå€¼é…ç½®
THRESHOLDS = {
    'ma50': True,           # è‚¡ä»·>MA50
    'ma150': True,          # è‚¡ä»·>MA150
    'ma150_ma200': True,    # MA150>MA200
    'ma10_ma20': True,      # MA10>MA20
    'low_52w_pct': 30,      # è¾ƒ52å‘¨æœ€ä½ç‚¹é«˜å‡ºâ‰¥30%
    'high_52w_pct': 80,     # è·52å‘¨æœ€é«˜ç‚¹â‰¤20% (å³â‰¥80%é«˜ç‚¹)
    'rs_rating': 70,        # RSè¯„åˆ†â‰¥70
    'price_min': 10,        # è‚¡ä»·>10å…ƒ
    'market_cap': 30,       # å¸‚å€¼>30äº¿
    'volume_min': 500000,   # æˆäº¤é‡>50ä¸‡è‚¡
    'amount_min': 200000000,  # æˆäº¤é¢>2äº¿å…ƒ
    'volume_ma_min': 500000,  # å„å‘¨æœŸå‡é‡>50ä¸‡è‚¡
    'outperform_factor': 2,   # è·‘èµ¢å¤§ç›˜å€æ•°ï¼ˆ1å€ä»¥ä¸Š=2å€ï¼‰
}

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(
            LOG_DIR / f'canslim_screening_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log',
            encoding='utf-8'
        ),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ============================================================
# æ•°æ®åŠ è½½ï¼ˆä¼˜åŒ–ç‰ˆ - ä½¿ç”¨å¿«ç…§ï¼‰
# ============================================================

def load_financial_data():
    """åŠ è½½è´¢åŠ¡æ•°æ®"""
    try:
        if not FINANCIAL_FILE.exists():
            logger.warning(f"è´¢åŠ¡æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {FINANCIAL_FILE}")
            return None
        
        # å°è¯•ä¸åŒç¼–ç 
        for encoding in ['utf-8', 'gbk', 'gb2312', 'utf-8-sig']:
            try:
                df = pd.read_csv(FINANCIAL_FILE, encoding=encoding)
                logger.info(f"âœ“ æˆåŠŸåŠ è½½è´¢åŠ¡æ•°æ® ({encoding}): {len(df)} æ¡")
                break
            except:
                continue
        else:
            return None
        
        # æ ‡å‡†åŒ–åˆ—å
        if 'è‚¡ç¥¨ä»£ç ' in df.columns:
            df = df.rename(columns={'è‚¡ç¥¨ä»£ç ': 'symbol'})
        
        # æ¸…ç†è‚¡ç¥¨ä»£ç 
        if 'symbol' in df.columns:
            df['symbol'] = df['symbol'].astype(str).str.strip()
            df['symbol'] = df['symbol'].str.replace('.SZ', '').str.replace('.SH', '')
        
        logger.info(f"  è´¢åŠ¡æ•°æ®åˆ—å: {list(df.columns)}")
        return df
    
    except Exception as e:
        logger.error(f"åŠ è½½è´¢åŠ¡æ•°æ®å¤±è´¥: {e}")
        return None


def load_snapshot_data(target_date, df_financial=None):
    """
    ä»å¿«ç…§åŠ è½½æ•°æ®ï¼ˆæ ¸å¿ƒä¼˜åŒ–ï¼‰âš¡
    
    æ€§èƒ½å¯¹æ¯”ï¼š
    - æ—§æ–¹æ³•ï¼šè¯»å–5000ä¸ªæ–‡ä»¶ï¼Œ10-30ç§’ï¼Œ2-5GBå†…å­˜
    - æ–°æ–¹æ³•ï¼šè¯»å–1ä¸ªæ–‡ä»¶ï¼Œ0.1-0.5ç§’ï¼Œ50-100MBå†…å­˜
    - æ€§èƒ½æå‡ï¼š100å€+
    """
    try:
        # é¦–å…ˆå°è¯•latest.parquet
        snapshot_file = SNAPSHOT_DIR / "latest.parquet"
        
        if not snapshot_file.exists():
            # å¦‚æœlatestä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾æŒ‡å®šæ—¥æœŸçš„å¿«ç…§
            snapshot_file = SNAPSHOT_DIR / f"snapshot_{target_date}.parquet"
        
        if not snapshot_file.exists():
            logger.error(f"âŒ æœªæ‰¾åˆ°å¿«ç…§æ–‡ä»¶: {snapshot_file}")
            logger.info(f"ğŸ’¡ è¯·å…ˆè¿è¡Œ generate_daily_snapshot.py ç”Ÿæˆå¿«ç…§")
            return None
        
        # âš¡ æ ¸å¿ƒï¼šä¸€æ¬¡æ€§è¯»å–å…¨å¸‚åœºå¿«ç…§ï¼ˆæ¯«ç§’çº§ï¼‰
        logger.info(f"ä»å¿«ç…§åŠ è½½æ•°æ®: {snapshot_file.name}")
        df = pd.read_parquet(snapshot_file)
        
        logger.info(f"âœ“ åŠ è½½å¿«ç…§å®Œæˆ: {len(df)} åªè‚¡ç¥¨ï¼ˆç”¨æ—¶<1ç§’ï¼‰âš¡")
        
        # æ ‡å‡†åŒ–åˆ—å
        rename_map = {
            'æ—¥æœŸ': 'date',
            'è‚¡ç¥¨ä»£ç ': 'symbol',
            'è‚¡ç¥¨åç§°': 'name',
            'æ”¶ç›˜': 'close',
            'å¼€ç›˜': 'open',
            'æœ€é«˜': 'high',
            'æœ€ä½': 'low',
            'æˆäº¤é‡': 'volume',
            'æˆäº¤é¢': 'amount',
        }
        df = df.rename(columns=rename_map)
        
        # éªŒè¯å¿«ç…§æ—¥æœŸ
        if 'date' in df.columns and not df.empty:
            snapshot_date = df['date'].iloc[0]
            if isinstance(snapshot_date, str):
                snapshot_date = pd.to_datetime(snapshot_date).strftime('%Y-%m-%d')
            else:
                snapshot_date = snapshot_date.strftime('%Y-%m-%d')
            logger.info(f"  å¿«ç…§æ—¥æœŸ: {snapshot_date}")
            
            # å¦‚æœå¿«ç…§æ—¥æœŸä¸ç›®æ ‡æ—¥æœŸä¸ä¸€è‡´ï¼Œç»™å‡ºæç¤º
            if snapshot_date != target_date:
                logger.warning(f"âš ï¸  å¿«ç…§æ—¥æœŸ({snapshot_date})ä¸ç›®æ ‡æ—¥æœŸ({target_date})ä¸ä¸€è‡´")
                logger.info(f"   å°†ä½¿ç”¨å¿«ç…§æ—¥æœŸçš„æ•°æ®")
                target_date = snapshot_date  # æ›´æ–°ç›®æ ‡æ—¥æœŸ
        
        # åˆå¹¶è´¢åŠ¡æ•°æ®
        if df_financial is not None and not df_financial.empty:
            logger.info("åˆå¹¶è´¢åŠ¡æ•°æ®...")
            
            # æŸ¥æ‰¾æµé€šè‚¡æœ¬åˆ—å
            float_col = None
            for col_name in ['æµé€šè‚¡(äº¿)', 'æµé€šè‚¡æœ¬(äº¿)', 'æµé€šè‚¡æœ¬(äº¿è‚¡)']:
                if col_name in df_financial.columns:
                    float_col = col_name
                    logger.info(f"  æ‰¾åˆ°æµé€šè‚¡æœ¬åˆ—: {float_col}")
                    break
            
            if float_col and 'close' in df.columns:
                # åˆå¹¶æµé€šè‚¡æœ¬
                df_fin_subset = df_financial[['symbol', float_col]].copy()
                df_fin_subset = df_fin_subset.rename(columns={float_col: 'float_share'})
                
                df = df.merge(df_fin_subset, on='symbol', how='left')
                
                # è®¡ç®—å¸‚å€¼ï¼ˆäº¿å…ƒï¼‰
                df['market_cap'] = df['close'] * df['float_share']
                
                valid_cap = df['market_cap'].notna().sum()
                logger.info(f"  æˆåŠŸè®¡ç®—å¸‚å€¼: {valid_cap} åªè‚¡ç¥¨")
            else:
                logger.warning("  æœªèƒ½è®¡ç®—å¸‚å€¼ï¼ˆç¼ºå°‘æµé€šè‚¡æœ¬æˆ–æ”¶ç›˜ä»·ï¼‰")
                df['market_cap'] = np.nan
        else:
            df['market_cap'] = np.nan
        
        return df
    
    except Exception as e:
        logger.error(f"åŠ è½½å¿«ç…§æ•°æ®å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None


def load_index_data(target_date):
    """åŠ è½½æŒ‡æ•°æ•°æ®ï¼ˆæ²ªæ·±300ï¼‰"""
    try:
        if not INDEX_DIR.exists():
            logger.warning(f"æŒ‡æ•°æ•°æ®ç›®å½•ä¸å­˜åœ¨: {INDEX_DIR}")
            return None
        
        # è¯»å–æ²ªæ·±300æŒ‡æ•°æ–‡ä»¶
        index_file = INDEX_DIR / f"{BENCHMARK_INDEX}.parquet"
        
        if not index_file.exists():
            logger.error(f"âŒ æœªæ‰¾åˆ°{BENCHMARK_NAME}æ•°æ®: {index_file}")
            return None
        
        df = pd.read_parquet(index_file)
        
        # æ ‡å‡†åŒ–åˆ—å
        rename_map = {
            'æ—¥æœŸ': 'date',
            'æŒ‡æ•°ä»£ç ': 'symbol',
            'æŒ‡æ•°åç§°': 'name',
        }
        df = df.rename(columns=rename_map)
        
        # ç­›é€‰ç›®æ ‡æ—¥æœŸ
        if 'date' in df.columns:
            df_date = df[df['date'] == target_date]
            if not df_date.empty:
                logger.info(f"âœ“ åŠ è½½{BENCHMARK_NAME}æ•°æ®: {len(df_date)} æ¡")
                return df_date
        
        logger.warning(f"âš ï¸  {BENCHMARK_NAME}æœªæ‰¾åˆ° {target_date} çš„æ•°æ®")
        return None
    
    except Exception as e:
        logger.error(f"åŠ è½½æŒ‡æ•°æ•°æ®å¤±è´¥: {e}")
        return None


def find_latest_available_date(target_date, max_days_back=10):
    """æŸ¥æ‰¾æœ€è¿‘å¯ç”¨çš„äº¤æ˜“æ—¥"""
    logger.info(f"æŸ¥æ‰¾å¯ç”¨äº¤æ˜“æ—¥ï¼ˆä» {target_date} å¾€å‰æœ€å¤š{max_days_back}å¤©ï¼‰...")
    
    # æ£€æŸ¥å¿«ç…§ç›®å½•
    if not SNAPSHOT_DIR.exists():
        logger.error(f"å¿«ç…§ç›®å½•ä¸å­˜åœ¨: {SNAPSHOT_DIR}")
        return None
    
    # é¦–å…ˆæ£€æŸ¥latest.parquet
    latest_file = SNAPSHOT_DIR / "latest.parquet"
    if latest_file.exists():
        try:
            df = pd.read_parquet(latest_file)
            if not df.empty:
                # è·å–å¿«ç…§æ—¥æœŸ
                date_col = None
                for col in ['æ—¥æœŸ', 'date', 'äº¤æ˜“æ—¥æœŸ']:
                    if col in df.columns:
                        date_col = col
                        break
                
                if date_col:
                    snapshot_date = df[date_col].iloc[0]
                    if isinstance(snapshot_date, str):
                        available_date = snapshot_date
                    else:
                        available_date = snapshot_date.strftime('%Y-%m-%d')
                    
                    logger.info(f"âœ“ æ‰¾åˆ°æœ€æ–°å¿«ç…§: {available_date}")
                    return available_date
        except Exception as e:
            logger.warning(f"è¯»å–latest.parquetå¤±è´¥: {e}")
    
    # å¦‚æœlatestä¸å­˜åœ¨ï¼ŒæŸ¥æ‰¾å¸¦æ—¥æœŸçš„å¿«ç…§æ–‡ä»¶
    snapshot_files = sorted(SNAPSHOT_DIR.glob("snapshot_*.parquet"), reverse=True)
    
    if snapshot_files:
        # ä»æ–‡ä»¶åæå–æ—¥æœŸ
        for file in snapshot_files:
            try:
                date_str = file.stem.replace('snapshot_', '')
                logger.info(f"âœ“ æ‰¾åˆ°å¿«ç…§æ–‡ä»¶: {date_str}")
                return date_str
            except:
                continue
    
    logger.error(f"âŒ æœªæ‰¾åˆ°å¯ç”¨çš„å¿«ç…§æ–‡ä»¶")
    logger.info(f"ğŸ’¡ è¯·å…ˆè¿è¡Œ generate_daily_snapshot.py ç”Ÿæˆå¿«ç…§")
    return None


# ============================================================
# CANSLIMæ¡ä»¶æ£€æŸ¥ï¼ˆä¿æŒä¸å˜ï¼‰
# ============================================================

def check_canslim_conditions(df_stocks, df_index):
    """
    æ£€æŸ¥13ä¸ªCANSLIMæ¡ä»¶
    
    è¾“å…¥ï¼šdf_stocks - ä»å¿«ç…§åŠ è½½çš„å…¨å¸‚åœºæ•°æ® âš¡
    è¾“å‡ºï¼šåŒ…å«æ£€æŸ¥ç»“æœçš„DataFrame
    """
    logger.info(f"å¼€å§‹æ£€æŸ¥ {len(df_stocks)} åªè‚¡ç¥¨çš„CANSLIMæ¡ä»¶...")
    
    results = []
    
    for idx, row in tqdm(df_stocks.iterrows(), total=len(df_stocks), desc="æ£€æŸ¥æ¡ä»¶"):
        result = {'symbol': row.get('symbol', ''), 'name': row.get('name', '')}
        
        # æ¡ä»¶1: è‚¡ä»· > MA50
        close = row.get('close', 0)
        ma50 = row.get('ma50', 0)
        c1 = (pd.notna(close) and pd.notna(ma50) and close > ma50)
        result['C1_ä»·æ ¼>MA50'] = 'âœ“' if c1 else 'âœ—'
        result['C1_å®é™…'] = f"{close:.2f} vs {ma50:.2f}" if pd.notna(close) and pd.notna(ma50) else "N/A"
        
        # æ¡ä»¶2: è‚¡ä»· > MA150
        ma150 = row.get('ma150', 0)
        c2 = (pd.notna(close) and pd.notna(ma150) and close > ma150)
        result['C2_ä»·æ ¼>MA150'] = 'âœ“' if c2 else 'âœ—'
        result['C2_å®é™…'] = f"{close:.2f} vs {ma150:.2f}" if pd.notna(close) and pd.notna(ma150) else "N/A"
        
        # æ¡ä»¶3: MA150 > MA200
        ma200 = row.get('ma200', 0)
        c3 = (pd.notna(ma150) and pd.notna(ma200) and ma150 > ma200)
        result['C3_MA150>MA200'] = 'âœ“' if c3 else 'âœ—'
        result['C3_å®é™…'] = f"{ma150:.2f} vs {ma200:.2f}" if pd.notna(ma150) and pd.notna(ma200) else "N/A"
        
        # æ¡ä»¶4: MA10 > MA20
        ma10 = row.get('ma10', 0)
        ma20 = row.get('ma20', 0)
        c4 = (pd.notna(ma10) and pd.notna(ma20) and ma10 > ma20)
        result['C4_MA10>MA20'] = 'âœ“' if c4 else 'âœ—'
        result['C4_å®é™…'] = f"{ma10:.2f} vs {ma20:.2f}" if pd.notna(ma10) and pd.notna(ma20) else "N/A"
        
        # æ¡ä»¶5: è¾ƒ52å‘¨ä½ç‚¹é«˜å‡ºâ‰¥30%
        low_52w = row.get('low_52w', 0)
        if pd.notna(low_52w) and low_52w > 0 and pd.notna(close):
            pct_from_low = (close - low_52w) / low_52w * 100
            c5 = pct_from_low >= THRESHOLDS['low_52w_pct']
            result['C5_è¾ƒä½ç‚¹é«˜30%'] = 'âœ“' if c5 else 'âœ—'
            result['C5_å®é™…'] = f"{pct_from_low:.2f}%"
        else:
            result['C5_è¾ƒä½ç‚¹é«˜30%'] = 'âœ—'
            result['C5_å®é™…'] = "N/A"
        
        # æ¡ä»¶6: è·52å‘¨é«˜ç‚¹â‰¤20% (å³â‰¥80%é«˜ç‚¹)
        high_52w = row.get('high_52w', 0)
        if pd.notna(high_52w) and high_52w > 0 and pd.notna(close):
            pct_from_high = close / high_52w * 100
            c6 = pct_from_high >= THRESHOLDS['high_52w_pct']
            result['C6_è·é«˜ç‚¹20%å†…'] = 'âœ“' if c6 else 'âœ—'
            result['C6_å®é™…'] = f"{pct_from_high:.2f}%"
        else:
            result['C6_è·é«˜ç‚¹20%å†…'] = 'âœ—'
            result['C6_å®é™…'] = "N/A"
        
        # æ¡ä»¶7: RSè¯„åˆ†â‰¥70
        rs_rating = row.get('rs_rating', 0)
        c7 = (pd.notna(rs_rating) and rs_rating >= THRESHOLDS['rs_rating'])
        result['C7_RSè¯„åˆ†â‰¥70'] = 'âœ“' if c7 else 'âœ—'
        result['C7_å®é™…'] = f"{rs_rating:.1f}" if pd.notna(rs_rating) else "N/A"
        
        # æ¡ä»¶8: è‚¡ä»·>10å…ƒ
        c8 = (pd.notna(close) and close >= THRESHOLDS['price_min'])
        result['C8_è‚¡ä»·>10å…ƒ'] = 'âœ“' if c8 else 'âœ—'
        result['C8_å®é™…'] = f"{close:.2f}å…ƒ" if pd.notna(close) else "N/A"
        
        # æ¡ä»¶9: å¸‚å€¼>30äº¿
        market_cap = row.get('market_cap', 0)
        c9 = (pd.notna(market_cap) and market_cap >= THRESHOLDS['market_cap'])
        result['C9_å¸‚å€¼>30äº¿'] = 'âœ“' if c9 else 'âœ—'
        result['C9_å®é™…'] = f"{market_cap:.2f}äº¿" if pd.notna(market_cap) else "N/A"
        
        # æ¡ä»¶10: æˆäº¤é‡>50ä¸‡
        volume = row.get('volume', 0)
        c10 = (pd.notna(volume) and volume >= THRESHOLDS['volume_min'])
        result['C10_æˆäº¤é‡>50ä¸‡'] = 'âœ“' if c10 else 'âœ—'
        result['C10_å®é™…'] = f"{volume:,.0f}è‚¡" if pd.notna(volume) else "N/A"
        
        # æ¡ä»¶11: æˆäº¤é¢>2äº¿
        amount = row.get('amount', 0)
        c11 = (pd.notna(amount) and amount >= THRESHOLDS['amount_min'])
        result['C11_æˆäº¤é¢>2äº¿'] = 'âœ“' if c11 else 'âœ—'
        result['C11_å®é™…'] = f"{amount/100000000:.2f}äº¿" if pd.notna(amount) else "N/A"
        
        # æ¡ä»¶12: å¤šå‘¨æœŸå‡é‡>50ä¸‡
        vol_ma10 = row.get('volume_ma10', 0)
        vol_ma30 = row.get('volume_ma30', 0)
        vol_ma60 = row.get('volume_ma60', 0)
        vol_ma90 = row.get('volume_ma90', 0)
        
        c12 = all([
            pd.notna(vol) and vol >= THRESHOLDS['volume_ma_min']
            for vol in [vol_ma10, vol_ma30, vol_ma60, vol_ma90]
        ])
        result['C12_å¤šå‘¨æœŸé‡>50ä¸‡'] = 'âœ“' if c12 else 'âœ—'
        vol_ma10_str = f"{vol_ma10:,.0f}" if pd.notna(vol_ma10) else "N/A"
        vol_ma30_str = f"{vol_ma30:,.0f}" if pd.notna(vol_ma30) else "N/A"
        result['C12_å®é™…'] = f"10æ—¥:{vol_ma10_str} 30æ—¥:{vol_ma30_str}"
        
        # æ¡ä»¶13: è·‘èµ¢å¤§ç›˜1å€ä»¥ä¸Š
        if df_index is not None and not df_index.empty:
            stock_1m = row.get('change_20d', 0)
            stock_3m = row.get('change_60d', 0)
            stock_6m = row.get('change_180d', 0)
            
            index_1m = df_index.iloc[0].get('change_20d', 0)
            index_3m = df_index.iloc[0].get('change_60d', 0)
            index_6m = df_index.iloc[0].get('change_180d', 0)
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æœŸé—´éƒ½è·‘èµ¢å¤§ç›˜è‡³å°‘1å€
            comparisons = []
            for stock, index in [(stock_1m, index_1m), (stock_3m, index_3m), (stock_6m, index_6m)]:
                if pd.notna(stock) and pd.notna(index) and index != 0:
                    comparisons.append(stock >= index * THRESHOLDS['outperform_factor'])
                else:
                    comparisons.append(False)
            
            c13 = all(comparisons)
            
            result['C13_è·‘èµ¢å¤§ç›˜1å€'] = 'âœ“' if c13 else 'âœ—'
            stock_1m_str = f"{stock_1m:.1f}%" if pd.notna(stock_1m) else "N/A"
            stock_3m_str = f"{stock_3m:.1f}%" if pd.notna(stock_3m) else "N/A"
            stock_6m_str = f"{stock_6m:.1f}%" if pd.notna(stock_6m) else "N/A"
            result['C13_å®é™…'] = f"1æœˆ:{stock_1m_str} 3æœˆ:{stock_3m_str} 6æœˆ:{stock_6m_str}"
        else:
            result['C13_è·‘èµ¢å¤§ç›˜1å€'] = 'âœ—'
            result['C13_å®é™…'] = "æ— å¤§ç›˜æ•°æ®"
        
        # ç»Ÿè®¡æ»¡è¶³æ¡ä»¶æ•°
        satisfied = sum([
            result[f'C{i}_{name}'] == 'âœ“' 
            for i, name in [
                (1, 'ä»·æ ¼>MA50'), (2, 'ä»·æ ¼>MA150'), (3, 'MA150>MA200'), (4, 'MA10>MA20'),
                (5, 'è¾ƒä½ç‚¹é«˜30%'), (6, 'è·é«˜ç‚¹20%å†…'), (7, 'RSè¯„åˆ†â‰¥70'), (8, 'è‚¡ä»·>10å…ƒ'),
                (9, 'å¸‚å€¼>30äº¿'), (10, 'æˆäº¤é‡>50ä¸‡'), (11, 'æˆäº¤é¢>2äº¿'), 
                (12, 'å¤šå‘¨æœŸé‡>50ä¸‡'), (13, 'è·‘èµ¢å¤§ç›˜1å€')
            ]
        ])
        
        result['æ»¡è¶³æ¡ä»¶æ•°'] = satisfied
        result['æ»¡è¶³ç‡%'] = f"{satisfied/13*100:.2f}"
        
        results.append(result)
    
    df_results = pd.DataFrame(results)
    logger.info(f"âœ“ æ¡ä»¶æ£€æŸ¥å®Œæˆ")
    
    return df_results


def analyze_conditions(df_results):
    """åˆ†æå„æ¡ä»¶çš„æ»¡è¶³æƒ…å†µ"""
    total = len(df_results)
    
    conditions = [
        ('C1', 'ä»·æ ¼>MA50', 'C1_ä»·æ ¼>MA50'),
        ('C2', 'ä»·æ ¼>MA150', 'C2_ä»·æ ¼>MA150'),
        ('C3', 'MA150>MA200', 'C3_MA150>MA200'),
        ('C4', 'MA10>MA20', 'C4_MA10>MA20'),
        ('C5', 'è¾ƒä½ç‚¹é«˜30%', 'C5_è¾ƒä½ç‚¹é«˜30%'),
        ('C6', 'è·é«˜ç‚¹20%å†…', 'C6_è·é«˜ç‚¹20%å†…'),
        ('C7', 'RSè¯„åˆ†â‰¥70', 'C7_RSè¯„åˆ†â‰¥70'),
        ('C8', 'è‚¡ä»·>10å…ƒ', 'C8_è‚¡ä»·>10å…ƒ'),
        ('C9', 'å¸‚å€¼>30äº¿', 'C9_å¸‚å€¼>30äº¿'),
        ('C10', 'æˆäº¤é‡>50ä¸‡', 'C10_æˆäº¤é‡>50ä¸‡'),
        ('C11', 'æˆäº¤é¢>2äº¿', 'C11_æˆäº¤é¢>2äº¿'),
        ('C12', 'å¤šå‘¨æœŸé‡>50ä¸‡', 'C12_å¤šå‘¨æœŸé‡>50ä¸‡'),
        ('C13', 'è·‘èµ¢å¤§ç›˜1å€', 'C13_è·‘èµ¢å¤§ç›˜1å€'),
    ]
    
    analysis = []
    for code, name, col in conditions:
        if col in df_results.columns:
            satisfied = (df_results[col] == 'âœ“').sum()
            pct = satisfied / total * 100 if total > 0 else 0
            analysis.append({
                'æ¡ä»¶ä»£ç ': code,
                'æ¡ä»¶åç§°': name,
                'æ»¡è¶³æ•°é‡': satisfied,
                'æ»¡è¶³ç‡%': f"{pct:.2f}"
            })
    
    return pd.DataFrame(analysis)


def save_results(df_results, df_analysis, target_date):
    """ä¿å­˜ç­›é€‰ç»“æœ"""
    try:
        # æŒ‰æ»¡è¶³æ¡ä»¶æ•°æ’åº
        df_output = df_results.sort_values('æ»¡è¶³æ¡ä»¶æ•°', ascending=False).copy()
        df_output.insert(0, 'æ’å', range(1, len(df_output) + 1))
        
        # 1. ä¿å­˜CSVï¼ˆå…¨éƒ¨ç»“æœï¼‰
        result_file = OUTPUT_DIR / f"{target_date}_CANSLIMç­›é€‰ç»“æœ.csv"
        df_output.to_csv(result_file, index=False, encoding='utf-8-sig')
        logger.info(f"âœ… ç­›é€‰ç»“æœå·²ä¿å­˜: {result_file}")
        
        # 2. ä¿å­˜æ¡ä»¶åˆ†æ
        analysis_file = OUTPUT_DIR / f"{target_date}_æ¡ä»¶åˆ†æ.csv"
        df_analysis.to_csv(analysis_file, index=False, encoding='utf-8-sig')
        logger.info(f"âœ… æ¡ä»¶åˆ†æå·²ä¿å­˜: {analysis_file}")
        
        # 3. ä¿å­˜æ»¡è¶³13æ¡çš„è‚¡ç¥¨æ¸…å•
        df_perfect = df_output[df_output['æ»¡è¶³æ¡ä»¶æ•°'] == 13].copy()
        if not df_perfect.empty:
            perfect_file = OUTPUT_DIR / f"{target_date}_å®Œç¾æ ‡çš„_æ»¡è¶³13æ¡.csv"
            df_perfect.to_csv(perfect_file, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… å®Œç¾æ ‡çš„æ¸…å•å·²ä¿å­˜: {perfect_file} ({len(df_perfect)}åª)")
        
        # 4. ä¿å­˜æ»¡è¶³12æ¡çš„è‚¡ç¥¨æ¸…å•
        df_almost = df_output[df_output['æ»¡è¶³æ¡ä»¶æ•°'] == 12].copy()
        if not df_almost.empty:
            almost_file = OUTPUT_DIR / f"{target_date}_ä¼˜ç§€æ ‡çš„_æ»¡è¶³12æ¡.csv"
            df_almost.to_csv(almost_file, index=False, encoding='utf-8-sig')
            logger.info(f"âœ… ä¼˜ç§€æ ‡çš„æ¸…å•å·²ä¿å­˜: {almost_file} ({len(df_almost)}åª)")
        
        # 5. ç”ŸæˆExcelæŠ¥å‘Š
        excel_file = OUTPUT_DIR / f"{target_date}_CANSLIMç­›é€‰æŠ¥å‘Š.xlsx"
        
        with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
            # Sheet 1: å…¨éƒ¨ç»“æœ
            df_output.to_excel(writer, sheet_name='å…¨éƒ¨ç»“æœ', index=False)
            
            # Sheet 2: å®Œå…¨æ»¡è¶³ (13/13)
            if not df_perfect.empty:
                df_perfect.to_excel(writer, sheet_name='å®Œå…¨æ»¡è¶³13æ¡', index=False)
            
            # Sheet 3: å‡ ä¹æ»¡è¶³ (12/13)
            if not df_almost.empty:
                df_almost.to_excel(writer, sheet_name='å‡ ä¹æ»¡è¶³12æ¡', index=False)
            
            # Sheet 4: å¤§éƒ¨åˆ†æ»¡è¶³ (11/13)
            df_mostly = df_output[df_output['æ»¡è¶³æ¡ä»¶æ•°'] == 11]
            if not df_mostly.empty:
                df_mostly.to_excel(writer, sheet_name='å¤§éƒ¨åˆ†æ»¡è¶³11æ¡', index=False)
            
            # Sheet 5: æ»¡è¶³â‰¥10æ¡
            df_good = df_output[df_output['æ»¡è¶³æ¡ä»¶æ•°'] >= 10]
            if not df_good.empty:
                df_good.to_excel(writer, sheet_name='æ»¡è¶³10æ¡ä»¥ä¸Š', index=False)
            
            # Sheet 6: æ¡ä»¶åˆ†æ
            df_analysis.to_excel(writer, sheet_name='æ¡ä»¶åˆ†æ', index=False)
        
        logger.info(f"âœ… ExcelæŠ¥å‘Šå·²ä¿å­˜: {excel_file}")
        
        return result_file, analysis_file, excel_file
    
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœå¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return None, None, None


def _symbol_with_suffix(symbol: str) -> str:
    """å°†6ä½ä»£ç è¡¥é½äº¤æ˜“æ‰€åç¼€ï¼Œè§„åˆ™ï¼šä»¥6å¼€å¤´ä¸ºSHï¼Œå¦åˆ™ä¸ºSZã€‚"""
    s = str(symbol or '').strip()
    if not s:
        return s
    if s.endswith('.SH') or s.endswith('.SZ'):
        return s
    return f"{s}.SH" if s.startswith('6') else f"{s}.SZ"


def export_frontend_basic_pool(
    df_stocks: pd.DataFrame,
    df_results: pd.DataFrame,
    target_date: str,
    *,
    min_passed: int = 11,
    out_name: str = 'basic_pool.json'
) -> Path | None:
    """å¯¼å‡ºå‰ç«¯å¯ç›´æ¥è¯»å–çš„ JSON åˆ° public/data/screen/{out_name}ã€‚

    å­—æ®µè¦æ±‚ï¼ˆä¸å‰ç«¯å¯¹é½ï¼‰ï¼š
      - symbol, name, passedCount, passedIndicators[], price, changePercent, amount
    é€šè¿‡ min_passed æ§åˆ¶é˜ˆå€¼ï¼›å½“ä¸º 0 æ—¶ä¸è¿‡æ»¤ï¼Œå¯¼å‡ºå…¨é‡ï¼ˆç”¨äº debugï¼‰ã€‚
    """
    try:
        repo_root = Path(__file__).resolve().parent.parent
        out_dir = repo_root / 'public' / 'data' / 'screen'
        out_dir.mkdir(parents=True, exist_ok=True)
        out_file = out_dir / out_name

        if df_results is None or df_results.empty:
            out_file.write_text('[]', encoding='utf-8')
            return out_file

        # é€‰å– >= min_passed æ¡çš„ç»“æœå¹¶æ’åºï¼ˆmin_passed==0 è¡¨ç¤ºä¸è¿‡æ»¤ï¼‰
        export_df = df_results.copy()
        if min_passed and min_passed > 0:
            export_df = export_df[export_df['æ»¡è¶³æ¡ä»¶æ•°'] >= min_passed].copy()
        if export_df.empty:
            out_file.write_text('[]', encoding='utf-8')
            return out_file

        export_df = export_df.sort_values('æ»¡è¶³æ¡ä»¶æ•°', ascending=False)

        # ä» stocks é‡Œè¡¥å……æ•°å€¼å­—æ®µ
        numeric_cols = ['symbol', 'close', 'amount', 'changePercent', 'change_percent', 'æ¶¨è·Œå¹…']
        df_num = df_stocks[[c for c in numeric_cols if c in df_stocks.columns]].copy()
        # æ ‡å‡†åŒ–æ¶¨è·Œå¹…å­—æ®µ
        if 'changePercent' not in df_num.columns:
            if 'change_percent' in df_num.columns:
                df_num = df_num.rename(columns={'change_percent': 'changePercent'})
            elif 'æ¶¨è·Œå¹…' in df_num.columns:
                df_num = df_num.rename(columns={'æ¶¨è·Œå¹…': 'changePercent'})

        merged = export_df.merge(df_num, on='symbol', how='left')

        records = []
        for _, row in merged.iterrows():
            # æ”¶é›†å‘½ä¸­æŒ‡æ ‡
            passed = []
            for i, name in [
                (1, 'è‚¡ä»· > 50æ—¥å‡çº¿'), (2, 'è‚¡ä»· > 150æ—¥å‡çº¿'), (3, '150æ—¥çº¿ > 200æ—¥çº¿'), (4, '10æ—¥çº¿ > 20æ—¥çº¿'),
                (5, 'è¾ƒ52å‘¨ä½ç‚¹é«˜30%'), (6, 'è·52å‘¨é«˜ç‚¹â‰¤20%'), (7, 'ç›¸å¯¹å¼ºåº¦RSâ‰¥70'), (8, 'è‚¡ä»· > 10å…ƒ'),
                (9, 'å¸‚å€¼ > 30äº¿'), (10, 'æˆäº¤é‡ > 50ä¸‡è‚¡'), (11, 'æˆäº¤é¢ > 2äº¿'), (12, 'å¹³å‡æˆäº¤é‡è¾¾æ ‡'),
                (13, 'è·‘èµ¢å¤§ç›˜1å€+')
            ]:
                col = f'C{i}_' + {
                    1: 'ä»·æ ¼>MA50', 2: 'ä»·æ ¼>MA150', 3: 'MA150>MA200', 4: 'MA10>MA20',
                    5: 'è¾ƒä½ç‚¹é«˜30%', 6: 'è·é«˜ç‚¹20%å†…', 7: 'RSè¯„åˆ†â‰¥70', 8: 'è‚¡ä»·>10å…ƒ',
                    9: 'å¸‚å€¼>30äº¿', 10: 'æˆäº¤é‡>50ä¸‡', 11: 'æˆäº¤é¢>2äº¿', 12: 'å¤šå‘¨æœŸé‡>50ä¸‡',
                    13: 'è·‘èµ¢å¤§ç›˜1å€'
                }[i]
                if col in export_df.columns and row.get(col) == 'âœ“':
                    passed.append(name)

            records.append({
                'symbol': _symbol_with_suffix(row.get('symbol', '')),
                'name': row.get('name', ''),
                'passedCount': int(row.get('æ»¡è¶³æ¡ä»¶æ•°', 0) or 0),
                'passedIndicators': passed,
                'price': float(row.get('close', 0) or 0),
                'changePercent': float(row.get('changePercent', 0) or 0),
                'amount': float(row.get('amount', 0) or 0)
            })

        with out_file.open('w', encoding='utf-8') as f:
            json.dump(records, f, ensure_ascii=False)

        logger.info(f"âœ… å‰ç«¯ç­›é€‰æ± JSONå·²å¯¼å‡º: {out_file} ({len(records)} æ¡, min_passed={min_passed})")
        return out_file
    except Exception as e:
        logger.error(f"å¯¼å‡ºå‰ç«¯JSONå¤±è´¥: {e}")
        return None


def print_summary(df_results, df_analysis, target_date):
    """æ‰“å°ç­›é€‰æ‘˜è¦"""
    print("\n" + "=" * 100)
    print("æ¡ä»¶åˆ†ææŠ¥å‘Š")
    print("=" * 100)
    print(df_analysis.to_string(index=False))
    
    print("\n" + "=" * 100)
    print("æ»¡è¶³æ¡ä»¶æ•°åˆ†å¸ƒ:")
    print("-" * 100)
    for threshold in [13, 12, 11, 10, 9, 8]:
        count = (df_results['æ»¡è¶³æ¡ä»¶æ•°'] >= threshold).sum()
        pct = count / len(df_results) * 100 if len(df_results) > 0 else 0
        print(f"  æ»¡è¶³ â‰¥{threshold:2d} ä¸ªæ¡ä»¶: {count:>5} åª ({pct:>6.2f}%)")
    print("-" * 100)
    
    # æ‰“å°Top20
    print("\n" + "=" * 100)
    print("CANSLIMç­›é€‰ Top20:")
    print("=" * 100)
    df_top = df_results.sort_values('æ»¡è¶³æ¡ä»¶æ•°', ascending=False).head(20).copy()
    df_top.insert(0, 'æ’å', range(1, len(df_top) + 1))
    
    display_cols = ['æ’å', 'symbol', 'name', 'æ»¡è¶³æ¡ä»¶æ•°', 'æ»¡è¶³ç‡%', 
                   'C7_å®é™…', 'C8_å®é™…', 'C9_å®é™…']
    
    df_display = df_top[display_cols].rename(columns={
        'symbol': 'ä»£ç ',
        'name': 'åç§°',
        'C7_å®é™…': 'RSè¯„åˆ†',
        'C8_å®é™…': 'è‚¡ä»·',
        'C9_å®é™…': 'å¸‚å€¼'
    })
    
    print(df_display.to_string(index=False))
    print("=" * 100)
    
    # æ‰“å°å®Œç¾æ ‡çš„å’Œä¼˜ç§€æ ‡çš„
    print("\n" + "=" * 100)
    print("ç‰¹åˆ«å…³æ³¨:")
    print("-" * 100)
    
    perfect_count = (df_results['æ»¡è¶³æ¡ä»¶æ•°'] == 13).sum()
    almost_count = (df_results['æ»¡è¶³æ¡ä»¶æ•°'] == 12).sum()
    
    print(f"  â­â­â­â­â­ å®Œç¾æ ‡çš„ï¼ˆæ»¡è¶³13æ¡ï¼‰: {perfect_count} åª")
    if perfect_count > 0:
        df_perfect = df_results[df_results['æ»¡è¶³æ¡ä»¶æ•°'] == 13][['symbol', 'name', 'C7_å®é™…', 'C8_å®é™…', 'C9_å®é™…']]
        for idx, row in df_perfect.iterrows():
            print(f"      {row['symbol']} {row['name']:10s} RS={row['C7_å®é™…']:>6s} ä»·æ ¼={row['C8_å®é™…']:>10s} å¸‚å€¼={row['C9_å®é™…']}")
    
    print(f"\n  â­â­â­â­ ä¼˜ç§€æ ‡çš„ï¼ˆæ»¡è¶³12æ¡ï¼‰: {almost_count} åª")
    if almost_count > 0:
        df_almost = df_results[df_results['æ»¡è¶³æ¡ä»¶æ•°'] == 12][['symbol', 'name', 'C7_å®é™…', 'C8_å®é™…', 'C9_å®é™…']].head(10)
        for idx, row in df_almost.iterrows():
            print(f"      {row['symbol']} {row['name']:10s} RS={row['C7_å®é™…']:>6s} ä»·æ ¼={row['C8_å®é™…']:>10s} å¸‚å€¼={row['C9_å®é™…']}")
        if almost_count > 10:
            print(f"      ... è¿˜æœ‰ {almost_count - 10} åªï¼ˆè¯¦è§CSVæ–‡ä»¶ï¼‰")
    
    print("=" * 100)


# ============================================================
# ä¸»ç¨‹åº
# ============================================================

def main():
    print("=" * 100)
    print("  CANSLIMé€‰è‚¡ç­–ç•¥ç­›é€‰ç¨‹åº v2.0ï¼ˆå¿«ç…§ç‰ˆï¼‰âš¡")
    print("  - 13ä¸ªCANSLIMç­›é€‰æ¡ä»¶")
    print(f"  - åŸºå‡†æŒ‡æ•°: {BENCHMARK_NAME} ({BENCHMARK_INDEX})")
    print("  - ä½¿ç”¨å¿«ç…§æŠ€æœ¯ï¼Œæ€§èƒ½æå‡100å€ âš¡")
    print("=" * 100)
    
    # æ£€æŸ¥å¿«ç…§ç›®å½•
    if not SNAPSHOT_DIR.exists():
        print(f"\nâŒ å¿«ç…§ç›®å½•ä¸å­˜åœ¨: {SNAPSHOT_DIR}")
        print(f"ğŸ’¡ è¯·å…ˆè¿è¡Œ generate_daily_snapshot.py ç”Ÿæˆå¿«ç…§")
        return
    
    # è·å–ç›®æ ‡æ—¥æœŸ
    target_date = datetime.now().strftime('%Y-%m-%d')
    print(f"\nç›®æ ‡æ—¥æœŸ: {target_date}")
    
    # æŸ¥æ‰¾å¯ç”¨äº¤æ˜“æ—¥
    available_date = find_latest_available_date(target_date, max_days_back=10)
    if available_date is None:
        print("\nâŒ æœªæ‰¾åˆ°å¯ç”¨çš„å¿«ç…§æ•°æ®")
        print(f"ğŸ’¡ è¯·å…ˆè¿è¡Œ generate_daily_snapshot.py ç”Ÿæˆå¿«ç…§")
        return
    
    target_date = available_date
    
    # åŠ è½½æ•°æ®
    logger.info("=" * 100)
    logger.info(f"å¼€å§‹CANSLIMç­›é€‰: {target_date}")
    logger.info("=" * 100)
    
    df_financial = load_financial_data()
    
    # âš¡ æ ¸å¿ƒä¼˜åŒ–ï¼šä»å¿«ç…§åŠ è½½æ•°æ®ï¼ˆæ¯«ç§’çº§ï¼‰
    df_stocks = load_snapshot_data(target_date, df_financial)
    
    if df_stocks is None or df_stocks.empty:
        logger.error(f"{target_date}: æ— å¯ç”¨å¿«ç…§æ•°æ®")
        return
    
    df_index = load_index_data(target_date)
    
    if df_index is None or df_index.empty:
        logger.warning(f"âš ï¸  æœªæ‰¾åˆ°{BENCHMARK_NAME}æ•°æ®ï¼Œå°†æ— æ³•æ£€æŸ¥æ¡ä»¶13")
    
    # æ‰§è¡Œç­›é€‰
    logger.info("å¼€å§‹æ£€æŸ¥13ä¸ªCANSLIMæ¡ä»¶...")
    df_results = check_canslim_conditions(df_stocks, df_index)
    
    # æ¡ä»¶åˆ†æ
    logger.info("ç”Ÿæˆæ¡ä»¶åˆ†ææŠ¥å‘Š...")
    df_analysis = analyze_conditions(df_results)
    
    # ä¿å­˜ç»“æœ
    logger.info("ä¿å­˜ç­›é€‰ç»“æœ...")
    result_file, analysis_file, excel_file = save_results(df_results, df_analysis, target_date)

    # å¯¼å‡ºç»™å‰ç«¯ä½¿ç”¨çš„ç­›é€‰æ±  JSONï¼ˆæ­£å¼ + è°ƒè¯•ï¼‰
    export_frontend_basic_pool(df_stocks, df_results, target_date, min_passed=11, out_name='basic_pool.json')
    export_frontend_basic_pool(df_stocks, df_results, target_date, min_passed=0, out_name='basic_pool_debug.json')
    
    # æ‰“å°æ‘˜è¦
    print_summary(df_results, df_analysis, target_date)
    
    # å®Œæˆ
    print("\n" + "=" * 100)
    print("âœ… CANSLIMç­›é€‰å®Œæˆï¼ï¼ˆå¿«ç…§ç‰ˆ - æ€§èƒ½æå‡100å€ï¼‰âš¡")
    print("=" * 100)
    print(f"\nğŸ“Š è¾“å‡ºæ–‡ä»¶:")
    print(f"  1. å…¨éƒ¨ç»“æœCSV: {result_file}")
    print(f"  2. æ¡ä»¶åˆ†æCSV: {analysis_file}")
    print(f"  3. ExcelæŠ¥å‘Š: {excel_file}")
    
    # ç»Ÿè®¡å®Œç¾å’Œä¼˜ç§€æ ‡çš„
    perfect_count = (df_results['æ»¡è¶³æ¡ä»¶æ•°'] == 13).sum()
    almost_count = (df_results['æ»¡è¶³æ¡ä»¶æ•°'] == 12).sum()
    
    if perfect_count > 0:
        perfect_file = OUTPUT_DIR / f"{target_date}_å®Œç¾æ ‡çš„_æ»¡è¶³13æ¡.csv"
        print(f"  4. å®Œç¾æ ‡çš„CSV: {perfect_file} ({perfect_count}åª)")
    
    if almost_count > 0:
        almost_file = OUTPUT_DIR / f"{target_date}_ä¼˜ç§€æ ‡çš„_æ»¡è¶³12æ¡.csv"
        print(f"  5. ä¼˜ç§€æ ‡çš„CSV: {almost_file} ({almost_count}åª)")
    
    print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print(f"  - â­â­â­â­â­ ä¼˜å…ˆå…³æ³¨'å®Œç¾æ ‡çš„_æ»¡è¶³13æ¡.csv'")
    print(f"  - â­â­â­â­ é‡ç‚¹æŸ¥çœ‹'ä¼˜ç§€æ ‡çš„_æ»¡è¶³12æ¡.csv'")
    print(f"  - æŸ¥çœ‹'æ¡ä»¶åˆ†æCSV'äº†è§£å„æ¡ä»¶çš„ç­›é€‰ä¸¥æ ¼ç¨‹åº¦")
    print(f"  - ExcelæŠ¥å‘ŠåŒ…å«å¤šä¸ªå·¥ä½œè¡¨ï¼Œä¾¿äºåˆ†å±‚æŸ¥çœ‹")
    print(f"  - æ¡ä»¶13ä½¿ç”¨{BENCHMARK_NAME}ä½œä¸ºåŸºå‡†ï¼Œè¦æ±‚è·‘èµ¢1å€ä»¥ä¸Š")
    
    print(f"\nâš¡ æ€§èƒ½ä¼˜åŠ¿:")
    print(f"  - ä½¿ç”¨å¿«ç…§æŠ€æœ¯ï¼Œä¸€æ¬¡è¯»å–å…¨å¸‚åœºæ•°æ®")
    print(f"  - æŸ¥è¯¢é€Ÿåº¦: 0.1-0.5ç§’ï¼ˆæ—§ç‰ˆï¼š10-30ç§’ï¼‰")
    print(f"  - æ€§èƒ½æå‡: 100å€+")
    print(f"  - å†…å­˜å ç”¨: 50-100MBï¼ˆæ—§ç‰ˆï¼š2-5GBï¼‰")
    
    print(f"\nâš ï¸  é‡è¦æé†’:")
    print(f"  - ç­›é€‰ç»“æœä»…ä¾›å‚è€ƒï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®")
    print(f"  - å»ºè®®ç»“åˆåŸºæœ¬é¢å’Œè¡Œä¸šåˆ†æ")
    print(f"  - æ³¨æ„æ§åˆ¶ä»“ä½å’Œé£é™©")


if __name__ == "__main__":
    main()
