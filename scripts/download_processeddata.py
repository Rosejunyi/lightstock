#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¤æƒæ•°æ®å¤„ç†è„šæœ¬ V2 - é€‚é…GitHub Actions
æ”¯æŒå¢é‡æ›´æ–°å’Œæ™ºèƒ½ä¿®å¤
"""

import baostock as bs
import pandas as pd
import os
from tqdm import tqdm
from datetime import datetime, timedelta
from pathlib import Path

print("="*60)
print("  è‚¡ç¥¨å¤æƒæ•°æ®å¤„ç† V2 (å¢é‡æ›´æ–°æ¨¡å¼)")
print("="*60)
print()

# --- è·¯å¾„å®šä¹‰ ---
BASE_DIR = Path(__file__).parent.parent
RAW_DATA_PATH = BASE_DIR / "data" / "daily_parquet"
PROCESSED_DATA_PATH = BASE_DIR / "data" / "adjusted_parquet"

# ç¡®ä¿ç›®å½•å­˜åœ¨
PROCESSED_DATA_PATH.mkdir(parents=True, exist_ok=True)

# è·å–è‚¡ç¥¨æ–‡ä»¶åˆ—è¡¨ï¼ˆæ’é™¤æŒ‡æ•°ï¼‰
try:
    stock_files = sorted([
        f.stem for f in RAW_DATA_PATH.glob("*.parquet")
        if not (f.name.startswith('sh.') or f.name.startswith('sz.'))
    ])
    print(f"ğŸ“ æ‰¾åˆ° {len(stock_files)} ä¸ªè‚¡ç¥¨æ–‡ä»¶")
except Exception as e:
    print(f"âŒ è¯»å–åŸå§‹æ•°æ®ç›®å½•å¤±è´¥: {e}")
    stock_files = []

# æŒ‡æ•°åˆ—è¡¨ï¼ˆä¸éœ€è¦å¤æƒï¼Œç›´æ¥å¤åˆ¶ï¼‰
INDEX_LIST = ["sh.000300", "sh.000001", "sz.399001", "sz.399006"]

# --- ç™»å½•Baostock ---
print("\nğŸ” ç™»å½•Baostock...")
lg = bs.login()
if lg.error_code != '0':
    print(f"âŒ Baostockç™»å½•å¤±è´¥: {lg.error_msg}")
    exit(1)
print("âœ… ç™»å½•æˆåŠŸ")
print()


def process_stock_with_adjustment(code, raw_file_path, processed_file_path):
    """
    å¤„ç†å•ä¸ªè‚¡ç¥¨çš„å¤æƒæ•°æ®ï¼ˆå¢é‡æ›´æ–°ï¼‰
    """
    if not raw_file_path.exists():
        return
    
    # ç¡®å®šèµ·å§‹æ—¥æœŸ
    start_date = '1990-01-01'
    existing_df = None
    
    # æ£€æŸ¥æ˜¯å¦å·²æœ‰å¤„ç†è¿‡çš„æ•°æ®
    if processed_file_path.exists():
        try:
            existing_df = pd.read_parquet(processed_file_path)
            if not existing_df.empty:
                last_date = pd.to_datetime(existing_df['æ—¥æœŸ'].iloc[-1])
                # ä»æœ€åä¸€å¤©çš„ä¸‹ä¸€å¤©å¼€å§‹æ›´æ–°
                start_date = (last_date + timedelta(days=1)).strftime('%Y-%m-%d')
                
                # å¦‚æœèµ·å§‹æ—¥æœŸå·²ç»è¶…è¿‡ä»Šå¤©ï¼Œè·³è¿‡
                if start_date > datetime.now().strftime('%Y-%m-%d'):
                    return
        except Exception as e:
            tqdm.write(f"âš ï¸ è¯»å–å·²å¤„ç†æ–‡ä»¶å¤±è´¥ï¼Œå°†å…¨æ–°å¤„ç†: {e}")
            existing_df = None
    
    try:
        # è¯»å–åŸå§‹æ•°æ®
        df_raw = pd.read_parquet(raw_file_path)
        df_raw['æ—¥æœŸ'] = pd.to_datetime(df_raw['æ—¥æœŸ'])
        
        # ç­›é€‰éœ€è¦å¤„ç†çš„æ–°æ•°æ®
        new_raw_df = df_raw[df_raw['æ—¥æœŸ'] >= pd.to_datetime(start_date)].copy()
        
        if new_raw_df.empty:
            return
        
        new_raw_df.set_index('æ—¥æœŸ', inplace=True)
        
        # æ„å»ºbaostockä»£ç æ ¼å¼
        code_bs = ("sh." if code.startswith('6') else "sz.") + code
        
        # è·å–å‰å¤æƒæ•°æ®
        def get_adjust_data(flag):
            data_list = []
            rs = bs.query_history_k_data_plus(
                code_bs,
                "date,open,high,low,close",
                start_date=start_date,
                frequency="d",
                adjustflag=flag
            )
            while (rs.error_code == '0') and rs.next():
                data_list.append(rs.get_row_data())
            return pd.DataFrame(data_list, columns=rs.fields) if data_list else pd.DataFrame()
        
        # è·å–å‰å¤æƒï¼ˆadjustflag="2"ï¼‰
        df_qfq = get_adjust_data("2")
        if not df_qfq.empty:
            df_qfq.rename(columns={
                'date': 'æ—¥æœŸ',
                'open': 'qfq_å¼€ç›˜',
                'high': 'qfq_æœ€é«˜',
                'low': 'qfq_æœ€ä½',
                'close': 'qfq_æ”¶ç›˜'
            }, inplace=True)
            df_qfq['æ—¥æœŸ'] = pd.to_datetime(df_qfq['æ—¥æœŸ'])
            df_qfq.set_index('æ—¥æœŸ', inplace=True)
            for col in ['qfq_å¼€ç›˜', 'qfq_æœ€é«˜', 'qfq_æœ€ä½', 'qfq_æ”¶ç›˜']:
                df_qfq[col] = pd.to_numeric(df_qfq[col], errors='coerce')
        
        # è·å–åå¤æƒï¼ˆadjustflag="1"ï¼‰
        df_hfq = get_adjust_data("1")
        if not df_hfq.empty:
            df_hfq.rename(columns={
                'date': 'æ—¥æœŸ',
                'open': 'hfq_å¼€ç›˜',
                'high': 'hfq_æœ€é«˜',
                'low': 'hfq_æœ€ä½',
                'close': 'hfq_æ”¶ç›˜'
            }, inplace=True)
            df_hfq['æ—¥æœŸ'] = pd.to_datetime(df_hfq['æ—¥æœŸ'])
            df_hfq.set_index('æ—¥æœŸ', inplace=True)
            for col in ['hfq_å¼€ç›˜', 'hfq_æœ€é«˜', 'hfq_æœ€ä½', 'hfq_æ”¶ç›˜']:
                df_hfq[col] = pd.to_numeric(df_hfq[col], errors='coerce')
        
        # åˆå¹¶åŸå§‹æ•°æ®å’Œå¤æƒæ•°æ®
        newly_processed_df = pd.concat([new_raw_df, df_qfq, df_hfq], axis=1)
        newly_processed_df.reset_index(inplace=True)
        
        # å¦‚æœå­˜åœ¨æ—§æ•°æ®ï¼Œè¿›è¡Œåˆå¹¶
        if existing_df is not None:
            final_df = pd.concat([existing_df, newly_processed_df], ignore_index=True)
            final_df.drop_duplicates(subset=['æ—¥æœŸ'], keep='last', inplace=True)
            tqdm.write(f"  âœ… {code}: å¢é‡æ›´æ–° {len(newly_processed_df)} æ¡æ•°æ®")
        else:
            final_df = newly_processed_df
            tqdm.write(f"  âœ… {code}: å…¨æ–°å¤„ç† {len(final_df)} æ¡æ•°æ®")
        
        # ä¿å­˜
        final_df.to_parquet(processed_file_path, index=False)
        
    except Exception as e:
        tqdm.write(f"  âŒ {code} å¤„ç†å¤±è´¥: {e}")


def copy_index_file(index_code, raw_path, processed_path):
    """
    å¤åˆ¶æŒ‡æ•°æ–‡ä»¶ï¼ˆæŒ‡æ•°ä¸éœ€è¦å¤æƒï¼‰
    """
    raw_file = raw_path / f"{index_code}.parquet"
    processed_file = processed_path / f"{index_code}.parquet"
    
    if not raw_file.exists():
        tqdm.write(f"  âš ï¸ {index_code}: åŸå§‹æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    try:
        # ç›´æ¥å¤åˆ¶
        df = pd.read_parquet(raw_file)
        df.to_parquet(processed_file, index=False)
        tqdm.write(f"  âœ… {index_code}: å¤åˆ¶å®Œæˆï¼ˆæŒ‡æ•°æ— éœ€å¤æƒï¼‰")
    except Exception as e:
        tqdm.write(f"  âŒ {index_code}: å¤åˆ¶å¤±è´¥ - {e}")


# --- ä¸»æµç¨‹ ---
if stock_files:
    print("="*60)
    print("ğŸ“Š å¼€å§‹å¤„ç†è‚¡ç¥¨å¤æƒæ•°æ®")
    print("="*60)
    print()
    
    for stock_code in tqdm(stock_files, desc="å¤„ç†è‚¡ç¥¨"):
        raw_file = RAW_DATA_PATH / f"{stock_code}.parquet"
        processed_file = PROCESSED_DATA_PATH / f"{stock_code}.parquet"
        process_stock_with_adjustment(stock_code, raw_file, processed_file)

print()
print("="*60)
print("ğŸ“ˆ å¼€å§‹å¤„ç†æŒ‡æ•°æ•°æ®")
print("="*60)
print()

for index_code in tqdm(INDEX_LIST, desc="å¤„ç†æŒ‡æ•°"):
    copy_index_file(index_code, RAW_DATA_PATH, PROCESSED_DATA_PATH)

# --- ç™»å‡º ---
bs.logout()

print()
print("="*60)
print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯")
print("="*60)

# ç»Ÿè®¡æ–‡ä»¶æ•°é‡
stock_count = len(list(PROCESSED_DATA_PATH.glob("*.parquet"))) - len(INDEX_LIST)
index_count = len([f for f in PROCESSED_DATA_PATH.glob("*.parquet") 
                   if f.stem in INDEX_LIST])

print(f"âœ… è‚¡ç¥¨æ–‡ä»¶: {stock_count}")
print(f"âœ… æŒ‡æ•°æ–‡ä»¶: {index_count}")
print(f"âœ… æ€»æ–‡ä»¶æ•°: {stock_count + index_count}")
print(f"âœ… è¾“å‡ºç›®å½•: {PROCESSED_DATA_PATH}")
print()
print("ğŸ‰ æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆï¼")
